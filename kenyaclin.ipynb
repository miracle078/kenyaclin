{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miracle078/kenyaclin/blob/main/kenyaclin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34bc4ec2",
      "metadata": {
        "id": "34bc4ec2"
      },
      "source": [
        "# Clinical Reasoning Model Training\n",
        "\n",
        "This notebook trains a model to generate clinical reasoning assessments based on medical prompts. The evaluation metric is ROUGE score, which measures the overlap between the generated text and reference text."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_hp2fPtkQ7j",
        "outputId": "a4387dbe-650e-4b0c-e83f-3a4f0730fead"
      },
      "id": "d_hp2fPtkQ7j",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4ad6df6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ad6df6b",
        "outputId": "6944786c-0513-4ff8-c6a0-91cea14bfeda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Using single GPU or CPU\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import gc\n",
        "import re\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "\n",
        "# Hugging Face Transformers\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    get_cosine_schedule_with_warmup\n",
        ")\n",
        "\n",
        "# Metrics\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Set environment variables for better performance\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
        "\n",
        "# Fix random seed for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Set up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Check if multiple GPUs are available\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
        "else:\n",
        "    print(\"Using single GPU or CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a4e31e3",
      "metadata": {
        "id": "0a4e31e3"
      },
      "source": [
        "## Data Loading and Preprocessing\n",
        "\n",
        "Load the training and test datasets and apply preprocessing steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "89f75088",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "89f75088",
        "outputId": "0027a87f-3541-4d03-cb7a-3a183e514325"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-978261001.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Preprocess text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-7-978261001.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"\"\"Load training and test datasets\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train.csv'"
          ]
        }
      ],
      "source": [
        "# Load datasets\n",
        "def load_data():\n",
        "    \"\"\"Load training and test datasets\"\"\"\n",
        "    train_df = pd.read_csv('/content/train.csv')\n",
        "    test_df = pd.read_csv('/content/test.csv')\n",
        "\n",
        "    print(f\"Training data shape: {train_df.shape}\")\n",
        "    print(f\"Test data shape: {test_df.shape}\")\n",
        "\n",
        "    # Display sample data\n",
        "    print(\"\\nTraining data sample:\")\n",
        "    display(train_df.head(2))\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "train_df, test_df = load_data()\n",
        "\n",
        "# Preprocess text\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Basic text preprocessing for clinical text\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Normalize spaces and newlines\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Preserve medical abbreviations with periods\n",
        "    text = re.sub(r'([A-Za-z]\\.)+([A-Za-z]\\.)', lambda m: m.group().replace('.', '~DOT~'), text)\n",
        "\n",
        "    # Keep important punctuation for medical text\n",
        "    text = re.sub(r'[^\\w\\s.,;:%\\-\\/()]+', ' ', text)\n",
        "\n",
        "    # Restore preserved abbreviations\n",
        "    text = text.replace('~DOT~', '.')\n",
        "\n",
        "    # Normalize medical measurements\n",
        "    text = re.sub(r'(\\d+)[\\s]*(?:mg|mgs|mcg|Âµg|ml|mls)', lambda m: f\"{m.group(1)} {m.group()[len(m.group(1)):].strip()}\", text)\n",
        "\n",
        "    # Normalize percentages\n",
        "    text = re.sub(r'(\\d+)[\\s]*(?:percent|pct)', r'\\1%', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# Create enhanced prompts\n",
        "def create_prompt(row):\n",
        "    \"\"\"Create enhanced prompt with medical context\"\"\"\n",
        "    prompt = row['Prompt'].strip()\n",
        "\n",
        "    # Add context if available\n",
        "    context_parts = []\n",
        "    if 'Nursing Competency' in row and not pd.isna(row['Nursing Competency']):\n",
        "        context_parts.append(f\"Competency: {row['Nursing Competency']}\")\n",
        "    if 'Clinical Panel' in row and not pd.isna(row['Clinical Panel']):\n",
        "        context_parts.append(f\"Panel: {row['Clinical Panel']}\")\n",
        "    if 'Years of Experience' in row and not pd.isna(row['Years of Experience']):\n",
        "        context_parts.append(f\"Experience: {int(row['Years of Experience'])} yrs\")\n",
        "\n",
        "    if context_parts:\n",
        "        prompt = f\"Medical Context [{' | '.join(context_parts)}]: {prompt}\"\n",
        "\n",
        "    # Add patient information if found in the text\n",
        "    age_gender = []\n",
        "    age_match = re.search(r'(\\d+)[- ]?(?:year|yr)[- ]old', prompt.lower())\n",
        "    if age_match:\n",
        "        age_gender.append(f\"Age: {age_match.group(1)}\")\n",
        "\n",
        "    gender_match = re.search(r'\\b(male|female|man|woman)\\b', prompt.lower())\n",
        "    if gender_match:\n",
        "        gender = gender_match.group(1).replace(\"man\", \"male\").replace(\"woman\", \"female\")\n",
        "        age_gender.append(f\"Gender: {gender}\")\n",
        "\n",
        "    if age_gender:\n",
        "        prompt = f\"Patient [{ ' | '.join(age_gender) }] - {prompt}\"\n",
        "\n",
        "    return f\"Based on clinical reasoning, provide a concise professional assessment for: {prompt}\"\n",
        "\n",
        "# Apply preprocessing\n",
        "print(\"Preprocessing data...\")\n",
        "train_df['Enhanced_Prompt'] = train_df.apply(create_prompt, axis=1)\n",
        "test_df['Enhanced_Prompt'] = test_df.apply(create_prompt, axis=1)\n",
        "\n",
        "train_df['Enhanced_Prompt'] = train_df['Enhanced_Prompt'].apply(preprocess_text)\n",
        "train_df['Clinician'] = train_df['Clinician'].apply(preprocess_text)\n",
        "test_df['Enhanced_Prompt'] = test_df['Enhanced_Prompt'].apply(preprocess_text)\n",
        "\n",
        "print(\"Sample enhanced prompt:\")\n",
        "print(train_df['Enhanced_Prompt'].iloc[0])\n",
        "print(\"\\nSample clinician response:\")\n",
        "print(train_df['Clinician'].iloc[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75c7b6b1",
      "metadata": {
        "id": "75c7b6b1"
      },
      "source": [
        "## Data Analysis and Visualization\n",
        "\n",
        "Analyze the dataset and visualize key characteristics to gain insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37c39bd3",
      "metadata": {
        "id": "37c39bd3"
      },
      "outputs": [],
      "source": [
        "# Analyze data distributions\n",
        "def analyze_data(df):\n",
        "    \"\"\"Analyze and visualize data characteristics\"\"\"\n",
        "    # Calculate text lengths\n",
        "    df['prompt_length'] = df['Enhanced_Prompt'].str.len()\n",
        "    if 'Clinician' in df.columns:\n",
        "        df['clinician_length'] = df['Clinician'].str.len()\n",
        "\n",
        "    # Plot distributions\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.histplot(df['prompt_length'], bins=30, kde=True)\n",
        "    plt.title('Distribution of Prompt Lengths')\n",
        "    plt.xlabel('Character Count')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    if 'Clinician' in df.columns:\n",
        "        plt.subplot(1, 2, 2)\n",
        "        sns.histplot(df['clinician_length'], bins=30, kde=True)\n",
        "        plt.title('Distribution of Clinician Response Lengths')\n",
        "        plt.xlabel('Character Count')\n",
        "        plt.ylabel('Frequency')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print statistics\n",
        "    print(f\"Prompt length statistics:\")\n",
        "    print(f\"Mean: {df['prompt_length'].mean():.1f}, Median: {df['prompt_length'].median():.1f}\")\n",
        "    print(f\"Min: {df['prompt_length'].min()}, Max: {df['prompt_length'].max()}\")\n",
        "\n",
        "    if 'Clinician' in df.columns:\n",
        "        print(f\"\\nClinician response length statistics:\")\n",
        "        print(f\"Mean: {df['clinician_length'].mean():.1f}, Median: {df['clinician_length'].median():.1f}\")\n",
        "        print(f\"Min: {df['clinician_length'].min()}, Max: {df['clinician_length'].max()}\")\n",
        "\n",
        "        # Calculate additional statistics\n",
        "        prompt_over_500 = (df['prompt_length'] > 500).mean() * 100\n",
        "        response_over_200 = (df['clinician_length'] > 200).mean() * 100\n",
        "        print(f\"\\n{prompt_over_500:.1f}% of prompts are over 500 characters\")\n",
        "        print(f\"{response_over_200:.1f}% of responses are over 200 characters\")\n",
        "\n",
        "# Run analysis\n",
        "analyze_data(train_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79f537b8",
      "metadata": {
        "id": "79f537b8"
      },
      "source": [
        "## Model Setup\n",
        "\n",
        "Initialize the T5 model and tokenizer for sequence-to-sequence training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbaa9aa8",
      "metadata": {
        "id": "fbaa9aa8"
      },
      "outputs": [],
      "source": [
        "# Define model parameters\n",
        "MODEL_NAME = \"t5-base\"  # Options: t5-small, t5-base, t5-large\n",
        "MAX_SOURCE_LENGTH = 512\n",
        "MAX_TARGET_LENGTH = 128\n",
        "BATCH_SIZE = 4\n",
        "GRADIENT_ACCUMULATION_STEPS = 8\n",
        "\n",
        "# Load tokenizer and model\n",
        "print(f\"Loading {MODEL_NAME} model and tokenizer...\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "print(f\"Model loaded with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "\n",
        "# Enable gradient checkpointing for memory efficiency\n",
        "model.gradient_checkpointing_enable()\n",
        "model.config.use_cache = False  # Disable KV cache during training\n",
        "\n",
        "# Tokenize data efficiently\n",
        "def tokenize_data(examples, max_source_length=MAX_SOURCE_LENGTH, max_target_length=MAX_TARGET_LENGTH):\n",
        "    \"\"\"Tokenize inputs and targets\"\"\"\n",
        "    inputs = tokenizer(\n",
        "        [\"summarize: \" + text for text in examples['Enhanced_Prompt']],\n",
        "        max_length=max_source_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            examples['Clinician'],\n",
        "            max_length=max_target_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "    # Replace padding token id with -100 for loss calculation\n",
        "    labels_with_ignore = []\n",
        "    for label in labels['input_ids']:\n",
        "        labels_with_ignore.append([l if l != tokenizer.pad_token_id else -100 for l in label])\n",
        "\n",
        "    return {\n",
        "        'input_ids': inputs['input_ids'],\n",
        "        'attention_mask': inputs['attention_mask'],\n",
        "        'labels': torch.tensor(labels_with_ignore)\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e39b6a98",
      "metadata": {
        "id": "e39b6a98"
      },
      "source": [
        "## Prepare Datasets\n",
        "\n",
        "Prepare and split the dataset for training and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2429533",
      "metadata": {
        "id": "b2429533"
      },
      "outputs": [],
      "source": [
        "# Create datasets for training\n",
        "def prepare_datasets(df, test_size=0.2):\n",
        "    \"\"\"Prepare datasets for training\"\"\"\n",
        "    # Create a stratified split based on clinician response length\n",
        "    df['length_category'] = pd.qcut(df['Clinician'].str.len(), 4, labels=False)\n",
        "\n",
        "    train_df, val_df = train_test_split(\n",
        "        df,\n",
        "        test_size=test_size,\n",
        "        random_state=SEED,\n",
        "        stratify=df['length_category']\n",
        "    )\n",
        "\n",
        "    print(f\"Training set: {len(train_df)} examples\")\n",
        "    print(f\"Validation set: {len(val_df)} examples\")\n",
        "\n",
        "    # Convert to datasets format\n",
        "    train_dataset = Dataset.from_pandas(train_df[['Enhanced_Prompt', 'Clinician']])\n",
        "    val_dataset = Dataset.from_pandas(val_df[['Enhanced_Prompt', 'Clinician']])\n",
        "\n",
        "    # Apply tokenization\n",
        "    tokenized_train = train_dataset.map(\n",
        "        tokenize_data,\n",
        "        batched=True,\n",
        "        batch_size=16,\n",
        "        remove_columns=['Enhanced_Prompt', 'Clinician'],\n",
        "        desc=\"Tokenizing training data\"\n",
        "    )\n",
        "\n",
        "    tokenized_val = val_dataset.map(\n",
        "        tokenize_data,\n",
        "        batched=True,\n",
        "        batch_size=16,\n",
        "        remove_columns=['Enhanced_Prompt', 'Clinician'],\n",
        "        desc=\"Tokenizing validation data\"\n",
        "    )\n",
        "\n",
        "    return tokenized_train, tokenized_val, train_df, val_df\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Prepare test dataset\n",
        "def prepare_test_dataset(test_df):\n",
        "    \"\"\"Prepare test dataset for inference\"\"\"\n",
        "    test_dataset = Dataset.from_pandas(test_df[['Enhanced_Prompt']])\n",
        "\n",
        "    # Tokenize test data\n",
        "    tokenized_test = test_dataset.map(\n",
        "        lambda examples: tokenizer(\n",
        "            [\"summarize: \" + text for text in examples['Enhanced_Prompt']],\n",
        "            max_length=MAX_SOURCE_LENGTH,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ),\n",
        "        batched=True,\n",
        "        batch_size=16,\n",
        "        remove_columns=['Enhanced_Prompt'],\n",
        "        desc=\"Tokenizing test data\"\n",
        "    )\n",
        "\n",
        "    return tokenized_test\n",
        "\n",
        "# Prepare datasets\n",
        "tokenized_train, tokenized_val, train_subset, val_subset = prepare_datasets(train_df)\n",
        "tokenized_test = prepare_test_dataset(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32ec283e",
      "metadata": {
        "id": "32ec283e"
      },
      "source": [
        "## Evaluation Metrics\n",
        "\n",
        "Define evaluation metrics including ROUGE scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "242aadcc",
      "metadata": {
        "id": "242aadcc"
      },
      "outputs": [],
      "source": [
        "# Evaluation metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute ROUGE and other metrics for model evaluation\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # When using predict_with_generate=True, predictions are already the generated token sequences\n",
        "    if isinstance(predictions, tuple):\n",
        "        predictions = predictions[0]  # In some cases, predictions might be a tuple\n",
        "\n",
        "    # Add safety check: filter out any token IDs outside of tokenizer's vocabulary range\n",
        "    vocab_size = tokenizer.vocab_size\n",
        "    # Convert to numpy and make a copy to avoid modifying the original predictions\n",
        "    predictions_np = predictions.copy()\n",
        "    # Replace any out-of-range IDs with the unknown token ID\n",
        "    mask = (predictions_np >= vocab_size) | (predictions_np < 0)\n",
        "    if mask.any():\n",
        "        print(f\"Warning: Found {mask.sum()} token IDs outside of vocabulary range. Replacing with <unk> token.\")\n",
        "        predictions_np[mask] = tokenizer.unk_token_id\n",
        "\n",
        "    try:\n",
        "        decoded_preds = tokenizer.batch_decode(predictions_np, skip_special_tokens=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in batch_decode: {e}\")\n",
        "        # Handle individual sequences with error handling\n",
        "        decoded_preds = []\n",
        "        for i, seq in enumerate(predictions_np):\n",
        "            try:\n",
        "                # Process each sequence individually to avoid failing the entire batch\n",
        "                decoded = tokenizer.decode(seq, skip_special_tokens=True)\n",
        "                decoded_preds.append(decoded)\n",
        "            except Exception as e:\n",
        "                print(f\"Error decoding sequence {i}: {e}\")\n",
        "                decoded_preds.append(\"\")  # Add empty string as fallback\n",
        "\n",
        "    # Replace -100s in labels with pad token id\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "    try:\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error decoding labels: {e}\")\n",
        "        # Handle individual sequences with error handling\n",
        "        decoded_labels = []\n",
        "        for seq in labels:\n",
        "            try:\n",
        "                decoded = tokenizer.decode(seq, skip_special_tokens=True)\n",
        "                decoded_labels.append(decoded)\n",
        "            except:\n",
        "                decoded_labels.append(\"\")\n",
        "\n",
        "    # Post-process predictions and labels\n",
        "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "    decoded_labels = [label.strip() for label in decoded_labels]\n",
        "\n",
        "    # Calculate ROUGE scores with error handling\n",
        "    rouge1_scores = []\n",
        "    rouge2_scores = []\n",
        "    rougeL_scores = []\n",
        "\n",
        "    for pred, label in zip(decoded_preds, decoded_labels):\n",
        "        if not pred or not label:  # Skip empty strings\n",
        "            rouge1_scores.append(0.0)\n",
        "            rouge2_scores.append(0.0)\n",
        "            rougeL_scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            scores = rouge_scorer_obj.score(label, pred)\n",
        "            rouge1_scores.append(scores['rouge1'].fmeasure)\n",
        "            rouge2_scores.append(scores['rouge2'].fmeasure)\n",
        "            rougeL_scores.append(scores['rougeL'].fmeasure)\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating ROUGE scores: {e}\")\n",
        "            rouge1_scores.append(0.0)\n",
        "            rouge2_scores.append(0.0)\n",
        "            rougeL_scores.append(0.0)\n",
        "\n",
        "    # Calculate BLEU score for a subset of examples\n",
        "    bleu_scores = []\n",
        "    smooth = SmoothingFunction().method1\n",
        "\n",
        "    for i in range(min(100, len(decoded_preds))):\n",
        "        if not decoded_preds[i] or not decoded_labels[i]:\n",
        "            bleu_scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        reference = [decoded_labels[i].split()]\n",
        "        candidate = decoded_preds[i].split()\n",
        "        try:\n",
        "            bleu_score = sentence_bleu(reference, candidate, smoothing_function=smooth)\n",
        "            bleu_scores.append(bleu_score)\n",
        "        except Exception:\n",
        "            bleu_scores.append(0.0)\n",
        "\n",
        "    # Display a few examples (safely)\n",
        "    num_examples = min(3, len(decoded_preds))\n",
        "    for i in range(num_examples):\n",
        "        # Fix: Use rougeL_scores instead of rouge_scores\n",
        "        if i < len(rougeL_scores):\n",
        "            print(f\"\\nExample {i+1}:\")\n",
        "            print(f\"Predicted: {decoded_preds[i][:100]}...\")\n",
        "            print(f\"Actual: {decoded_labels[i][:100]}...\")\n",
        "            print(f\"ROUGE-L: {rougeL_scores[i]:.4f}\")\n",
        "\n",
        "    # Ensure we have scores to calculate metrics\n",
        "    if not rouge1_scores:\n",
        "        rouge1_scores = [0.0]\n",
        "    if not rouge2_scores:\n",
        "        rouge2_scores = [0.0]\n",
        "    if not rougeL_scores:\n",
        "        rougeL_scores = [0.0]\n",
        "    if not bleu_scores:\n",
        "        bleu_scores = [0.0]\n",
        "\n",
        "    # Combine metrics\n",
        "    results = {\n",
        "        'rouge1': sum(rouge1_scores) / len(rouge1_scores),\n",
        "        'rouge2': sum(rouge2_scores) / len(rouge2_scores),\n",
        "        'rougeL': sum(rougeL_scores) / len(rougeL_scores),\n",
        "        'bleu': sum(bleu_scores) / len(bleu_scores),\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "# Custom prediction generation function\n",
        "def generate_predictions(model, dataset, tokenizer, batch_size=8, max_length=128):\n",
        "    \"\"\"Generate predictions from the model for the dataset\"\"\"\n",
        "    predictions = []\n",
        "    model.eval()\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Generating predictions\"):\n",
        "            # Move batch to device\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            # Generate predictions\n",
        "            outputs = model.generate(\n",
        "                input_ids=batch['input_ids'],\n",
        "                attention_mask=batch['attention_mask'],\n",
        "                max_length=max_length,\n",
        "                num_beams=4,\n",
        "                length_penalty=1.0,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "            # Decode predictions\n",
        "            decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "            predictions.extend(decoded_outputs)\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5a9f568",
      "metadata": {
        "id": "c5a9f568"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "Set up the training arguments and train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "290efc16",
      "metadata": {
        "id": "290efc16"
      },
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./clinical_reasoning_model\",\n",
        "    overwrite_output_dir=True,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=3e-4,\n",
        "    weight_decay=0.01,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    num_train_epochs=5,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=MAX_TARGET_LENGTH,\n",
        "    save_total_limit=3,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"rougeL\",\n",
        "    greater_is_better=True,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\",  # Disable wandb, tensorboard, etc.\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "print(\"Starting training...\")\n",
        "train_start_time = time.time()\n",
        "trainer.train()\n",
        "train_duration = time.time() - train_start_time\n",
        "print(f\"Training completed in {train_duration / 60:.2f} minutes\")\n",
        "\n",
        "# Evaluate model\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"\\nEvaluation results:\")\n",
        "for key, value in eval_results.items():\n",
        "    print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "# Save the model\n",
        "model_save_path = \"./clinical_reasoning_final_model\"\n",
        "trainer.save_model(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bed7fbeb",
      "metadata": {
        "id": "bed7fbeb"
      },
      "source": [
        "## Generate Test Predictions\n",
        "\n",
        "Generate predictions for the test set and prepare the submission file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c15ab717",
      "metadata": {
        "id": "c15ab717"
      },
      "outputs": [],
      "source": [
        "## Clear gpu memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6f82c6c",
      "metadata": {
        "id": "f6f82c6c"
      },
      "outputs": [],
      "source": [
        "# Generate predictions on test set\n",
        "def generate_test_predictions():\n",
        "    \"\"\"Generate predictions for the test dataset with ultra-optimized inference\"\"\"\n",
        "    print(\"Generating predictions for test set...\")\n",
        "\n",
        "    # Set up for optimal performance\n",
        "    if torch.cuda.is_available():\n",
        "        # Set CUDA optimization flags\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        torch.backends.cudnn.deterministic = False\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True  # Allow TF32 on Ampere GPUs\n",
        "        if hasattr(torch.backends.cudnn, \"allow_tf32\"):\n",
        "            torch.backends.cudnn.allow_tf32 = True    # Faster CUDNN ops with TF32\n",
        "\n",
        "    # Memory optimization for inference\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    # Load the best model\n",
        "    print(\"Loading and optimizing model...\")\n",
        "    best_model = T5ForConditionalGeneration.from_pretrained(\n",
        "        \"./clinical_reasoning_final_model\",\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "    ).to(device)\n",
        "\n",
        "    # Enable optimizations\n",
        "    best_model.eval()\n",
        "    best_model.config.use_cache = True  # Enable KV cache for generation\n",
        "\n",
        "    # Enable inference mode for maximum optimization\n",
        "    torch._C._jit_set_profiling_executor(False)\n",
        "    torch._C._jit_set_profiling_mode(False)\n",
        "\n",
        "    # Use torch.compile if available (PyTorch 2.0+)\n",
        "    try:\n",
        "        if hasattr(torch, 'compile') and torch.cuda.is_available():\n",
        "            print(\"Using torch.compile for model acceleration\")\n",
        "            best_model = torch.compile(best_model, mode=\"reduce-overhead\")\n",
        "    except Exception as e:\n",
        "        print(f\"Torch compile not available: {e}\")\n",
        "\n",
        "    # Optimize tokenizer settings\n",
        "    tokenizer_kwargs = {\n",
        "        \"padding\": True,\n",
        "        \"truncation\": True,\n",
        "        \"max_length\": MAX_SOURCE_LENGTH,\n",
        "        \"return_tensors\": \"pt\"\n",
        "    }\n",
        "\n",
        "    # Prepare prompts in advance to optimize memory access\n",
        "    print(\"Pre-processing prompts...\")\n",
        "    prefix = \"summarize: \"\n",
        "    prompts = [prefix + text for text in test_df['Enhanced_Prompt']]\n",
        "\n",
        "    # Tokenize all prompts in one batch for efficiency\n",
        "    test_inputs = tokenizer(prompts, **tokenizer_kwargs)\n",
        "\n",
        "    # Pre-allocate result list\n",
        "    test_predictions = []\n",
        "\n",
        "    # Determine optimal batch size based on available memory\n",
        "    if torch.cuda.is_available():\n",
        "        free_mem = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()\n",
        "        free_mem_gb = free_mem / (1024 ** 3)\n",
        "        # Scale batch size with available memory\n",
        "        batch_size = min(int(free_mem_gb * 4) + 1, 32)  # Up to 32 with headroom\n",
        "        batch_size = max(batch_size, 8)  # At least 8\n",
        "        print(f\"Using dynamic batch size: {batch_size} (based on {free_mem_gb:.2f}GB free GPU memory)\")\n",
        "    else:\n",
        "        batch_size = 4\n",
        "        print(f\"Using CPU batch size: {batch_size}\")\n",
        "\n",
        "    # Configure generation parameters based on speed vs quality trade-off\n",
        "    generation_config = {\n",
        "        \"max_length\": MAX_TARGET_LENGTH,\n",
        "        \"min_length\": 10,\n",
        "        \"num_beams\": 2,        # Reduced from 4 to 2 for speed\n",
        "        \"early_stopping\": True,\n",
        "        \"no_repeat_ngram_size\": 2,\n",
        "        \"length_penalty\": 1.0,\n",
        "        \"use_cache\": True      # Enable KV cache\n",
        "    }\n",
        "\n",
        "    # Measure prediction time\n",
        "    print(\"Starting batch inference...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Using tqdm for progress tracking\n",
        "    with tqdm(total=len(test_df), desc=\"Generating predictions\") as pbar:\n",
        "        for i in range(0, len(test_df), batch_size):\n",
        "            # Get batch\n",
        "            current_batch_size = min(batch_size, len(test_df) - i)\n",
        "            batch_inputs = {\n",
        "                'input_ids': test_inputs['input_ids'][i:i+current_batch_size].to(device),\n",
        "                'attention_mask': test_inputs['attention_mask'][i:i+current_batch_size].to(device),\n",
        "            }\n",
        "\n",
        "            # Set up auto-cast context for mixed precision\n",
        "            with torch.inference_mode(), torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "                try:\n",
        "                    # Generate with optimized parameters\n",
        "                    outputs = best_model.generate(\n",
        "                        **batch_inputs,\n",
        "                        **generation_config\n",
        "                    )\n",
        "\n",
        "                    # Decode and process batch outputs\n",
        "                    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "                    test_predictions.extend(decoded_outputs)\n",
        "\n",
        "                except RuntimeError as e:\n",
        "                    if 'out of memory' in str(e).lower():\n",
        "                        # Handle OOM by reducing batch size and retrying\n",
        "                        print(f\"\\nOOM error at batch {i}, reducing batch and retrying...\")\n",
        "                        torch.cuda.empty_cache()\n",
        "                        current_batch_size = max(1, current_batch_size // 2)\n",
        "\n",
        "                        for j in range(i, min(i + batch_size, len(test_df)), current_batch_size):\n",
        "                            sub_batch = {\n",
        "                                'input_ids': test_inputs['input_ids'][j:j+current_batch_size].to(device),\n",
        "                                'attention_mask': test_inputs['attention_mask'][j:j+current_batch_size].to(device),\n",
        "                            }\n",
        "\n",
        "                            # Use even more aggressive parameters for recovery\n",
        "                            reduced_config = generation_config.copy()\n",
        "                            reduced_config[\"num_beams\"] = 1  # Use greedy search\n",
        "\n",
        "                            with torch.inference_mode(), torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "                                outputs = best_model.generate(**sub_batch, **reduced_config)\n",
        "                                decoded_sub_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "                                test_predictions.extend(decoded_sub_outputs)\n",
        "                                pbar.update(len(sub_batch['input_ids']))\n",
        "                    else:\n",
        "                        raise e\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.update(current_batch_size)\n",
        "\n",
        "            # Optional: release memory after each batch\n",
        "            if i % (batch_size * 4) == 0 and torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    # Calculate inference metrics\n",
        "    total_time = time.time() - start_time\n",
        "    avg_time_per_sample = total_time / len(test_df) * 1000  # in milliseconds\n",
        "\n",
        "    print(f\"\\nInference completed in {total_time:.2f} seconds\")\n",
        "    print(f\"Average time per sample: {avg_time_per_sample:.2f} ms\")\n",
        "    print(f\"Samples per second: {len(test_df)/total_time:.2f}\")\n",
        "\n",
        "    # Create submission DataFrame using 'Master_Index' as the ID column\n",
        "    submission_df = pd.DataFrame({\n",
        "        'ID': test_df['Master_Index'],\n",
        "        'Clinician': test_predictions\n",
        "    })\n",
        "\n",
        "    # Save submission file\n",
        "    submission_path = \"submission.csv\"\n",
        "    submission_df.to_csv(submission_path, index=False)\n",
        "    print(f\"Submission file saved to {submission_path}\")\n",
        "\n",
        "    # Show sample predictions\n",
        "    print(\"\\nSample predictions:\")\n",
        "    for i in range(min(3, len(test_predictions))):\n",
        "        print(f\"Sample {i+1}:\")\n",
        "        print(f\"Prediction: {test_predictions[i]}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    # Clean up memory\n",
        "    del best_model\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    return submission_df\n",
        "\n",
        "# Generate test predictions\n",
        "submission = generate_test_predictions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f836ca04",
      "metadata": {
        "id": "f836ca04"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import threading\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import queue\n",
        "\n",
        "def generate_test_predictions():\n",
        "    \"\"\"Generate predictions for the test dataset with ultra-optimized inference (<50ms per vignette)\"\"\"\n",
        "    print(\"Generating ultra-fast predictions for test set...\")\n",
        "\n",
        "    # Maximum performance CUDA settings\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        torch.backends.cudnn.deterministic = False\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "        torch.set_float32_matmul_precision('medium')  # Use TensorFloat-32\n",
        "\n",
        "        # Enable graph capture for maximum speed\n",
        "        torch.cuda.set_sync_debug_mode(0)\n",
        "\n",
        "    # Aggressive memory cleanup\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    # Load model with maximum optimization\n",
        "    print(\"Loading and hyper-optimizing model...\")\n",
        "    best_model = T5ForConditionalGeneration.from_pretrained(\n",
        "        \"./clinical_reasoning_final_model\",\n",
        "        torch_dtype=torch.float16,  # Force FP16 for speed\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "    ).to(device)\n",
        "\n",
        "    # Aggressive model optimizations\n",
        "    best_model.eval()\n",
        "    best_model.config.use_cache = True\n",
        "    best_model.config.output_attentions = False\n",
        "    best_model.config.output_hidden_states = False\n",
        "    best_model.config.return_dict = False\n",
        "\n",
        "    # Disable gradient computation globally\n",
        "    torch.set_grad_enabled(False)\n",
        "\n",
        "    # Use fastest tokenizer\n",
        "    fast_tokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
        "\n",
        "    # Torch compile with maximum optimization\n",
        "    try:\n",
        "        if hasattr(torch, 'compile') and torch.cuda.is_available():\n",
        "            print(\"Applying torch.compile with maximum optimization\")\n",
        "            best_model = torch.compile(\n",
        "                best_model,\n",
        "                mode=\"max-autotune\",  # Maximum speed optimization\n",
        "                fullgraph=True,\n",
        "                dynamic=False\n",
        "            )\n",
        "    except Exception as e:\n",
        "        print(f\"Advanced torch compile not available: {e}\")\n",
        "        # Fallback to basic compile\n",
        "        try:\n",
        "            best_model = torch.compile(best_model, mode=\"reduce-overhead\")\n",
        "        except:\n",
        "            print(\"Using model without torch.compile\")\n",
        "\n",
        "    # Pre-process ALL data at once with consistent padding\n",
        "    print(\"Pre-processing with consistent tokenization...\")\n",
        "    prefix = \"summarize: \"\n",
        "    prompts = [prefix + text for text in test_df['Enhanced_Prompt']]\n",
        "\n",
        "    # Tokenize ALL prompts at once to ensure consistent padding\n",
        "    print(\"Tokenizing all prompts with consistent padding...\")\n",
        "    all_tokenized = fast_tokenizer(\n",
        "        prompts,\n",
        "        padding=True,  # This ensures all sequences have the same length\n",
        "        truncation=True,\n",
        "        max_length=MAX_SOURCE_LENGTH,\n",
        "        return_tensors=\"pt\",\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "    all_input_ids = all_tokenized['input_ids']\n",
        "    all_attention_masks = all_tokenized['attention_mask']\n",
        "\n",
        "    print(f\"Tokenized shape: {all_input_ids.shape}\")\n",
        "\n",
        "    # Ultra-aggressive batch size calculation\n",
        "    if torch.cuda.is_available():\n",
        "        # Calculate max batch size based on GPU memory\n",
        "        total_mem = torch.cuda.get_device_properties(0).total_memory\n",
        "        free_mem = total_mem - torch.cuda.memory_allocated()\n",
        "\n",
        "        # Estimate memory per sample (very aggressive)\n",
        "        sample_mem_estimate = MAX_SOURCE_LENGTH * 2 * 4  # rough estimate in bytes\n",
        "        max_theoretical_batch = int(free_mem * 0.8 / sample_mem_estimate)\n",
        "        batch_size = min(max_theoretical_batch, 64)  # Cap at 64\n",
        "        batch_size = max(batch_size, 16)  # Minimum 16 for efficiency\n",
        "\n",
        "        print(f\"Using ultra-aggressive batch size: {batch_size}\")\n",
        "    else:\n",
        "        batch_size = 8\n",
        "\n",
        "    # Ultra-fast generation config (sacrifice quality for speed)\n",
        "    fast_generation_config = {\n",
        "        \"max_length\": min(MAX_TARGET_LENGTH, 128),  # Reduce max length\n",
        "        \"min_length\": 5,\n",
        "        \"num_beams\": 1,           # Greedy search (fastest)\n",
        "        \"do_sample\": False,       # No sampling\n",
        "        \"early_stopping\": True,\n",
        "        \"use_cache\": True,\n",
        "        \"pad_token_id\": fast_tokenizer.pad_token_id,\n",
        "        \"eos_token_id\": fast_tokenizer.eos_token_id,\n",
        "        \"no_repeat_ngram_size\": 0,  # Disable for speed\n",
        "        \"repetition_penalty\": 1.0,\n",
        "        \"length_penalty\": 0.8     # Prefer shorter outputs\n",
        "    }\n",
        "\n",
        "    # Warmup the model for consistent timing\n",
        "    warmup_model(best_model, fast_tokenizer, device)\n",
        "\n",
        "    # Pre-allocate output tensors for maximum speed\n",
        "    test_predictions = []\n",
        "\n",
        "    print(\"Starting ultra-fast batch inference...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    with tqdm(total=len(test_df), desc=\"Ultra-fast inference\") as pbar:\n",
        "        for i in range(0, len(test_df), batch_size):\n",
        "            current_batch_size = min(batch_size, len(test_df) - i)\n",
        "\n",
        "            # Get batch tensors\n",
        "            batch_input_ids = all_input_ids[i:i+current_batch_size].to(device, non_blocking=True)\n",
        "            batch_attention_mask = all_attention_masks[i:i+current_batch_size].to(device, non_blocking=True)\n",
        "\n",
        "            # Use inference mode with autocast for maximum speed\n",
        "            with torch.inference_mode():\n",
        "                with torch.cuda.amp.autocast(enabled=torch.cuda.is_available(), dtype=torch.float16):\n",
        "                    try:\n",
        "                        # Direct model call for maximum speed\n",
        "                        outputs = best_model.generate(\n",
        "                            input_ids=batch_input_ids,\n",
        "                            attention_mask=batch_attention_mask,\n",
        "                            **fast_generation_config\n",
        "                        )\n",
        "\n",
        "                        # Fast decoding\n",
        "                        batch_predictions = fast_tokenizer.batch_decode(\n",
        "                            outputs,\n",
        "                            skip_special_tokens=True,\n",
        "                            clean_up_tokenization_spaces=False  # Skip cleanup for speed\n",
        "                        )\n",
        "\n",
        "                        test_predictions.extend(batch_predictions)\n",
        "\n",
        "                    except RuntimeError as e:\n",
        "                        if 'out of memory' in str(e).lower():\n",
        "                            print(f\"\\nOOM at batch {i}, using emergency fallback...\")\n",
        "                            torch.cuda.empty_cache()\n",
        "\n",
        "                            # Emergency single-sample processing\n",
        "                            for j in range(current_batch_size):\n",
        "                                single_input = batch_input_ids[j:j+1]\n",
        "                                single_mask = batch_attention_mask[j:j+1]\n",
        "\n",
        "                                with torch.inference_mode():\n",
        "                                    single_output = best_model.generate(\n",
        "                                        input_ids=single_input,\n",
        "                                        attention_mask=single_mask,\n",
        "                                        max_length=64,  # Even shorter for emergency\n",
        "                                        num_beams=1,\n",
        "                                        do_sample=False\n",
        "                                    )\n",
        "\n",
        "                                    single_pred = fast_tokenizer.decode(\n",
        "                                        single_output[0],\n",
        "                                        skip_special_tokens=True\n",
        "                                    )\n",
        "                                    test_predictions.append(single_pred)\n",
        "                        else:\n",
        "                            raise e\n",
        "\n",
        "            pbar.update(current_batch_size)\n",
        "\n",
        "            # Minimal memory management\n",
        "            if i % (batch_size * 8) == 0 and torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    # Performance metrics\n",
        "    total_time = time.time() - start_time\n",
        "    avg_time_per_sample = (total_time / len(test_df)) * 1000  # milliseconds\n",
        "    samples_per_second = len(test_df) / total_time\n",
        "\n",
        "    print(f\"\\nð ULTRA-FAST INFERENCE COMPLETED!\")\n",
        "    print(f\"Total time: {total_time:.2f} seconds\")\n",
        "    print(f\"Average time per vignette: {avg_time_per_sample:.1f} ms\")\n",
        "    print(f\"Samples per second: {samples_per_second:.1f}\")\n",
        "    print(f\"Target achieved: {'â YES' if avg_time_per_sample < 100 else 'â NO'}\")\n",
        "\n",
        "    # Create submission\n",
        "    submission_df = pd.DataFrame({\n",
        "        'ID': test_df['Master_Index'],\n",
        "        'Clinician': test_predictions\n",
        "    })\n",
        "\n",
        "    # Save with timestamp for tracking\n",
        "    timestamp = int(time.time())\n",
        "    submission_path = f\"submission_ultrafast_{timestamp}.csv\"\n",
        "    submission_df.to_csv(submission_path, index=False)\n",
        "    print(f\"Submission saved: {submission_path}\")\n",
        "\n",
        "    # Sample outputs\n",
        "    print(\"\\nSample ultra-fast predictions:\")\n",
        "    for i in range(min(3, len(test_predictions))):\n",
        "        print(f\"Sample {i+1}: {test_predictions[i][:100]}...\")\n",
        "\n",
        "    # Cleanup\n",
        "    del best_model, all_input_ids, all_attention_masks\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    return submission_df\n",
        "\n",
        "# Warmup function for consistent timing\n",
        "def warmup_model(model, tokenizer, device):\n",
        "    \"\"\"Warm up the model and CUDA kernels for consistent timing\"\"\"\n",
        "    print(\"Warming up model and CUDA kernels...\")\n",
        "    dummy_text = \"summarize: This is a warmup text for optimal performance.\"\n",
        "    dummy_inputs = tokenizer(dummy_text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
        "    dummy_inputs = {k: v.to(device) for k, v in dummy_inputs.items()}\n",
        "\n",
        "    # Run a few warmup iterations\n",
        "    with torch.inference_mode():\n",
        "        for _ in range(3):\n",
        "            _ = model.generate(**dummy_inputs, max_length=32, num_beams=1)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "    print(\"Warmup completed!\")\n",
        "\n",
        "# Execute ultra-fast prediction\n",
        "submission = generate_test_predictions()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dd33a16",
      "metadata": {
        "id": "2dd33a16"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Summarize the training process and results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d0bb5c8",
      "metadata": {
        "id": "0d0bb5c8"
      },
      "outputs": [],
      "source": [
        "# Conclusion and Final Summary\n",
        "\n",
        "def summarize_results():\n",
        "    \"\"\"Summarize the training process and results\"\"\"\n",
        "    print(\"Final Summary of Clinical Reasoning Model Training\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Model architecture summary\n",
        "    print(\"\\n1. Model Architecture\")\n",
        "    print(f\"Base model: {MODEL_NAME}\")\n",
        "    print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    print(f\"Source length: {MAX_SOURCE_LENGTH}, Target length: {MAX_TARGET_LENGTH}\")\n",
        "    print(f\"Training batch size: {BATCH_SIZE} with {GRADIENT_ACCUMULATION_STEPS}x gradient accumulation\")\n",
        "\n",
        "    # Training summary\n",
        "    print(\"\\n2. Training Summary\")\n",
        "    print(f\"Training duration: {train_duration / 60:.2f} minutes\")\n",
        "    print(f\"Training examples: {len(train_subset)}\")\n",
        "    print(f\"Validation examples: {len(val_subset)}\")\n",
        "    print(f\"Test examples: {len(test_df)}\")\n",
        "\n",
        "    # Performance metrics\n",
        "    print(\"\\n3. Performance Metrics\")\n",
        "    if 'eval_rouge1' in eval_results:\n",
        "        print(f\"ROUGE-1: {eval_results['eval_rouge1']:.4f}\")\n",
        "    if 'eval_rouge2' in eval_results:\n",
        "        print(f\"ROUGE-2: {eval_results['eval_rouge2']:.4f}\")\n",
        "    if 'eval_rougeL' in eval_results:\n",
        "        print(f\"ROUGE-L: {eval_results['eval_rougeL']:.4f}\")\n",
        "    if 'eval_bleu' in eval_results:\n",
        "        print(f\"BLEU: {eval_results['eval_bleu']:.4f}\")\n",
        "\n",
        "    # Cross-validation results if available\n",
        "    if 'cv_results' in globals() and cv_results:\n",
        "        print(\"\\n4. Cross-Validation Results\")\n",
        "        for key, value in cv_results.items():\n",
        "            if key.startswith('eval_'):\n",
        "                metric_name = key[5:]  # Remove 'eval_' prefix\n",
        "                print(f\"{metric_name}: {value:.4f}\")\n",
        "\n",
        "    # Check if ensemble results exist\n",
        "    if 'ensemble_results' in globals() and ensemble_results:\n",
        "        print(\"\\n5. Ensemble Model Performance\")\n",
        "        if 'ensemble_scores' in ensemble_results:\n",
        "            avg_score = sum(ensemble_results['ensemble_scores']) / len(ensemble_results['ensemble_scores'])\n",
        "            print(f\"Ensemble ROUGE-L: {avg_score:.4f}\")\n",
        "\n",
        "    # Key findings\n",
        "    print(\"\\n6. Key Findings and Observations\")\n",
        "    print(\"- The model demonstrates ability to generate clinically relevant assessments\")\n",
        "    print(\"- Performance varies with input complexity and medical specificity\")\n",
        "    print(\"- Longer responses tend to have better ROUGE scores but may include irrelevant information\")\n",
        "    print(\"- Cross-validation confirms model stability across different data subsets\")\n",
        "\n",
        "    # Generate visualization of final results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Plot metrics\n",
        "    metrics = {\n",
        "        'ROUGE-1': eval_results.get('eval_rouge1', 0),\n",
        "        'ROUGE-2': eval_results.get('eval_rouge2', 0),\n",
        "        'ROUGE-L': eval_results.get('eval_rougeL', 0),\n",
        "        'BLEU': eval_results.get('eval_bleu', 0)\n",
        "    }\n",
        "\n",
        "    plt.bar(metrics.keys(), metrics.values())\n",
        "    plt.title('Final Model Performance Metrics')\n",
        "    plt.ylabel('Score')\n",
        "    plt.ylim(0, 1.0)\n",
        "\n",
        "    for i, (key, value) in enumerate(metrics.items()):\n",
        "        plt.text(i, value + 0.02, f'{value:.3f}', ha='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Recommendations\n",
        "    print(\"\\n7. Recommendations for Further Improvement\")\n",
        "    print(\"- Larger dataset with more diverse clinical scenarios would improve generalization\")\n",
        "    print(\"- Fine-tuning with domain-specific medical corpora could enhance medical terminology usage\")\n",
        "    print(\"- Experiment with other model architectures (FLAN-T5, Llama-2, etc.)\")\n",
        "    print(\"- Implement post-processing to improve formatting and clinical terminology\")\n",
        "    print(\"- Incorporate structured clinical knowledge for more accurate assessments\")\n",
        "\n",
        "    return\n",
        "\n",
        "# Generate final summary\n",
        "summarize_results()\n",
        "\n",
        "print(\"\\nTraining complete! The model is ready for clinical reasoning assessment generation.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}