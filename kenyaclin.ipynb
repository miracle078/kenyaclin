{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miracle078/kenyaclin/blob/main/kenyaclin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34bc4ec2",
      "metadata": {
        "id": "34bc4ec2"
      },
      "source": [
        "# Clinical Reasoning Model Training\n",
        "\n",
        "This notebook trains a model to generate clinical reasoning assessments based on medical prompts. The evaluation metric is ROUGE score, which measures the overlap between the generated text and reference text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "30a618b9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in c:\\users\\mirac\\anaconda3\\lib\\site-packages (24.2)\n",
            "Collecting pip\n",
            "  Using cached pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Using cached pip-25.1.1-py3-none-any.whl (1.8 MB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: To modify pip, please run the following command:\n",
            "C:\\Users\\mirac\\anaconda3\\python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in c:\\users\\mirac\\anaconda3\\lib\\site-packages (2.7.1+cu118)\n",
            "Requirement already satisfied: torchvision in c:\\users\\mirac\\anaconda3\\lib\\site-packages (0.22.1+cu118)\n",
            "Requirement already satisfied: torchaudio in c:\\users\\mirac\\anaconda3\\lib\\site-packages (2.7.1+cu118)\n",
            "Requirement already satisfied: filelock in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: transformers==4.35.2 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from transformers[torch]==4.35.2) (4.35.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from transformers==4.35.2->transformers[torch]==4.35.2) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from transformers==4.35.2->transformers[torch]==4.35.2) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from transformers==4.35.2->transformers[torch]==4.35.2) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from transformers==4.35.2->transformers[torch]==4.35.2) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from transformers==4.35.2->transformers[torch]==4.35.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from transformers==4.35.2->transformers[torch]==4.35.2) (2024.9.11)\n",
            "Requirement already satisfied: requests in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from transformers==4.35.2->transformers[torch]==4.35.2) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from transformers==4.35.2->transformers[torch]==4.35.2) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from transformers==4.35.2->transformers[torch]==4.35.2) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from transformers==4.35.2->transformers[torch]==4.35.2) (4.66.5)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from transformers[torch]==4.35.2) (2.7.1+cu118)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from transformers[torch]==4.35.2) (0.24.1)\n",
            "Requirement already satisfied: psutil in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from accelerate>=0.20.3->transformers[torch]==4.35.2) (5.9.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2->transformers[torch]==4.35.2) (2023.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2->transformers[torch]==4.35.2) (4.11.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from torch!=1.12.0,>=1.10->transformers[torch]==4.35.2) (1.13.3)\n",
            "Requirement already satisfied: networkx in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from torch!=1.12.0,>=1.10->transformers[torch]==4.35.2) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from torch!=1.12.0,>=1.10->transformers[torch]==4.35.2) (3.1.4)\n",
            "Requirement already satisfied: setuptools in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from torch!=1.12.0,>=1.10->transformers[torch]==4.35.2) (75.1.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.35.2->transformers[torch]==4.35.2) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from requests->transformers==4.35.2->transformers[torch]==4.35.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from requests->transformers==4.35.2->transformers[torch]==4.35.2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from requests->transformers==4.35.2->transformers[torch]==4.35.2) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from requests->transformers==4.35.2->transformers[torch]==4.35.2) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch!=1.12.0,>=1.10->transformers[torch]==4.35.2) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]==4.35.2) (2.1.3)\n",
            "Requirement already satisfied: datasets==2.14.6 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (2.14.6)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from datasets==2.14.6) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from datasets==2.14.6) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from datasets==2.14.6) (0.3.7)\n",
            "Requirement already satisfied: pandas in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from datasets==2.14.6) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from datasets==2.14.6) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from datasets==2.14.6) (4.66.5)\n",
            "Requirement already satisfied: xxhash in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from datasets==2.14.6) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from datasets==2.14.6) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.14.6) (2023.10.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from datasets==2.14.6) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from datasets==2.14.6) (0.33.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from datasets==2.14.6) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from datasets==2.14.6) (6.0.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.14.6) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.14.6) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.14.6) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.14.6) (1.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.14.6) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.14.6) (1.11.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.6) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.6) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets==2.14.6) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets==2.14.6) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets==2.14.6) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets==2.14.6) (2025.4.26)\n",
            "Requirement already satisfied: colorama in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets==2.14.6) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from pandas->datasets==2.14.6) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from pandas->datasets==2.14.6) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from pandas->datasets==2.14.6) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.14.6) (1.16.0)\n",
            "Collecting tokenizers==0.15.0\n",
            "  Using cached tokenizers-0.15.0.tar.gz (318 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'error'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [22 lines of output]\n",
            "      Checking for Rust toolchain....\n",
            "      Rust not found, installing into a temporary directory\n",
            "      Python reports platform: win-amd64\n",
            "      Computed rustc target triple: x86_64-pc-windows-msvc\n",
            "      Installation directory: C:\\Users\\mirac\\AppData\\Local\\puccinialin\\puccinialin\\Cache\n",
            "      Rustup already downloaded\n",
            "      Installing rust to C:\\Users\\mirac\\AppData\\Local\\puccinialin\\puccinialin\\Cache\\rustup\n",
            "      warn: It looks like you have an existing rustup settings file at:\n",
            "      warn: C:\\Users\\mirac\\.rustup\\settings.toml\n",
            "      warn: Rustup will install the default toolchain as specified in the settings file,\n",
            "      warn: instead of the one inferred from the default host triple.\n",
            "      info: profile set to 'minimal'\n",
            "      info: default host triple is x86_64-pc-windows-msvc\n",
            "      warn: Updating existing toolchain, profile choice will be ignored\n",
            "      info: syncing channel updates for 'stable-x86_64-pc-windows-msvc'\n",
            "      info: default toolchain set to 'stable-x86_64-pc-windows-msvc'\n",
            "      Checking if cargo is installed\n",
            "      \n",
            "      Cargo, the Rust package manager, is not installed or is not on PATH.\n",
            "      This package requires Rust and Cargo to compile extensions. Install it through\n",
            "      the system's package manager or via https://rustup.rs/\n",
            "      \n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "error: metadata-generation-failed\n",
            "\n",
            "× Encountered error while generating package metadata.\n",
            "╰─> See above for output.\n",
            "\n",
            "note: This is an issue with the package mentioned above, not pip.\n",
            "hint: See above for details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate==0.24.1 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from accelerate==0.24.1) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from accelerate==0.24.1) (24.1)\n",
            "Requirement already satisfied: psutil in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from accelerate==0.24.1) (5.9.0)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from accelerate==0.24.1) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from accelerate==0.24.1) (2.7.1+cu118)\n",
            "Requirement already satisfied: huggingface-hub in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from accelerate==0.24.1) (0.33.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate==0.24.1) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate==0.24.1) (4.11.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate==0.24.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate==0.24.1) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate==0.24.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate==0.24.1) (2023.10.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate==0.24.1) (75.1.0)\n",
            "Requirement already satisfied: requests in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from huggingface-hub->accelerate==0.24.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from huggingface-hub->accelerate==0.24.1) (4.66.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.10.0->accelerate==0.24.1) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate==0.24.1) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate==0.24.1) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.24.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.24.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.24.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.24.1) (2025.4.26)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\mirac\\anaconda3\\lib\\site-packages (1.5.1)\n",
            "Requirement already satisfied: rouge-score in c:\\users\\mirac\\anaconda3\\lib\\site-packages (0.1.2)\n",
            "Requirement already satisfied: nltk in c:\\users\\mirac\\anaconda3\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\mirac\\anaconda3\\lib\\site-packages (3.9.2)\n",
            "Requirement already satisfied: seaborn in c:\\users\\mirac\\anaconda3\\lib\\site-packages (0.13.2)\n",
            "Requirement already satisfied: pandas in c:\\users\\mirac\\anaconda3\\lib\\site-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in c:\\users\\mirac\\anaconda3\\lib\\site-packages (1.26.4)\n",
            "Requirement already satisfied: tqdm in c:\\users\\mirac\\anaconda3\\lib\\site-packages (4.66.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: absl-py in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from rouge-score) (2.3.0)\n",
            "Requirement already satisfied: six>=1.14.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\mirac\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\mirac\\anaconda3\\lib\\site-packages (0.2.0)\n",
            "Requirement already satisfied: protobuf in c:\\users\\mirac\\anaconda3\\lib\\site-packages (4.25.3)\n"
          ]
        }
      ],
      "source": [
        "# Install all required packages with specific versions for compatibility\n",
        "!pip install --upgrade pip\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install transformers[torch]==4.35.2\n",
        "!pip install datasets==2.14.6\n",
        "!pip install tokenizers==0.15.0\n",
        "!pip install accelerate==0.24.1\n",
        "!pip install scikit-learn rouge-score nltk matplotlib seaborn pandas numpy tqdm\n",
        "!pip install sentencepiece protobuf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4ad6df6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ad6df6b",
        "outputId": "6944786c-0513-4ff8-c6a0-91cea14bfeda"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mirac\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "c:\\Users\\mirac\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "W0623 09:31:34.302000 29764 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
            "c:\\Users\\mirac\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Using single GPU or CPU\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import gc\n",
        "import re\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "\n",
        "# Hugging Face Transformers\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    get_cosine_schedule_with_warmup\n",
        ")\n",
        "\n",
        "# Metrics\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Set environment variables for better performance\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
        "\n",
        "# Fix random seed for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Set up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Check if multiple GPUs are available\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
        "else:\n",
        "    print(\"Using single GPU or CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a4e31e3",
      "metadata": {
        "id": "0a4e31e3"
      },
      "source": [
        "## Data Loading and Preprocessing\n",
        "\n",
        "Load the training and test datasets and apply preprocessing steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "89f75088",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "89f75088",
        "outputId": "0027a87f-3541-4d03-cb7a-3a183e514325"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (400, 12)\n",
            "Test data shape: (100, 7)\n",
            "\n",
            "Training data sample:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Master_Index</th>\n",
              "      <th>County</th>\n",
              "      <th>Health level</th>\n",
              "      <th>Years of Experience</th>\n",
              "      <th>Prompt</th>\n",
              "      <th>Nursing Competency</th>\n",
              "      <th>Clinical Panel</th>\n",
              "      <th>Clinician</th>\n",
              "      <th>GPT4.0</th>\n",
              "      <th>LLAMA</th>\n",
              "      <th>GEMINI</th>\n",
              "      <th>DDX SNOMED</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ID_VBWWP</td>\n",
              "      <td>uasin gishu</td>\n",
              "      <td>sub county hospitals and nursing homes</td>\n",
              "      <td>18.0</td>\n",
              "      <td>i am a nurse with 18 years of experience in ge...</td>\n",
              "      <td>pediatric emergency burns</td>\n",
              "      <td>surgery</td>\n",
              "      <td>summary a 4 year old with 5 superficial burns ...</td>\n",
              "      <td>given your vast experience as a nurse in uasin...</td>\n",
              "      <td>1 immediate treatment protocol for second degr...</td>\n",
              "      <td>here s a response addressing the questions reg...</td>\n",
              "      <td>288514009 burn involving 5 percent of body sur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ID_XMBBY</td>\n",
              "      <td>uasin gishu</td>\n",
              "      <td>national referral hospitals</td>\n",
              "      <td>17.0</td>\n",
              "      <td>i am a nurse with 17 years of experience in ge...</td>\n",
              "      <td>child health</td>\n",
              "      <td>paediatrics</td>\n",
              "      <td>summary 6 year old present with vomiting and a...</td>\n",
              "      <td>clinical summary • a 6 year old girl with know...</td>\n",
              "      <td>based on the symptoms and signs you ve describ...</td>\n",
              "      <td>based on the presentation the 6 year old girl ...</td>\n",
              "      <td>420270002 ketoacidosis due to type 1 diabetes ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Master_Index       County                            Health level  \\\n",
              "0     ID_VBWWP  uasin gishu  sub county hospitals and nursing homes   \n",
              "1     ID_XMBBY  uasin gishu             national referral hospitals   \n",
              "\n",
              "   Years of Experience                                             Prompt  \\\n",
              "0                 18.0  i am a nurse with 18 years of experience in ge...   \n",
              "1                 17.0  i am a nurse with 17 years of experience in ge...   \n",
              "\n",
              "          Nursing Competency Clinical Panel  \\\n",
              "0  pediatric emergency burns        surgery   \n",
              "1               child health    paediatrics   \n",
              "\n",
              "                                           Clinician  \\\n",
              "0  summary a 4 year old with 5 superficial burns ...   \n",
              "1  summary 6 year old present with vomiting and a...   \n",
              "\n",
              "                                              GPT4.0  \\\n",
              "0  given your vast experience as a nurse in uasin...   \n",
              "1  clinical summary • a 6 year old girl with know...   \n",
              "\n",
              "                                               LLAMA  \\\n",
              "0  1 immediate treatment protocol for second degr...   \n",
              "1  based on the symptoms and signs you ve describ...   \n",
              "\n",
              "                                              GEMINI  \\\n",
              "0  here s a response addressing the questions reg...   \n",
              "1  based on the presentation the 6 year old girl ...   \n",
              "\n",
              "                                          DDX SNOMED  \n",
              "0  288514009 burn involving 5 percent of body sur...  \n",
              "1  420270002 ketoacidosis due to type 1 diabetes ...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing data...\n",
            "Sample enhanced prompt:\n",
            "Based on clinical reasoning, provide a concise professional assessment for: Patient  Age: 4  - Medical Context  Competency: pediatric emergency burns   Panel: surgery   Experience: 18 yrs : i am a nurse with 18 years of experience in general nursing working in a sub county hospitals and nursing homes in uasin gishu county in kenya a 4 year old child presents to the emergency department with second degree burns on the forearm after accidentally touching a hot stove the child was playing in the kitchen when they reached out to touch the stove the burns cover about 5 of the total body surface area the child is alert and crying with redness blisters and swelling on the affected area the burns appear to be superficial to moderate in severity the child is in mild pain and there is no indication of airway or breathing distress no other injuries are noted questions 1 what is the immediate treatment protocol for second degree burns in paediatric patients 2 should any tetanus prophylaxis be considered in this case 3 what follow up care should be recommended for burn healing\n",
            "\n",
            "Sample clinician response:\n",
            "summary a 4 year old with 5 superficial burns no other injuries immediate management paracetamol analgesics to to ensure child has minimal or no pain cleaning and frosting of wound with silver sulpha fizika topical prophylactic can be considered in this case good nutrition high protein diet\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "def load_data():\n",
        "    \"\"\"Load training and test datasets\"\"\"\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "\n",
        "    print(f\"Training data shape: {train_df.shape}\")\n",
        "    print(f\"Test data shape: {test_df.shape}\")\n",
        "\n",
        "    # Display sample data\n",
        "    print(\"\\nTraining data sample:\")\n",
        "    display(train_df.head(2))\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "train_df, test_df = load_data()\n",
        "\n",
        "# Preprocess text\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Basic text preprocessing for clinical text\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Normalize spaces and newlines\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Preserve medical abbreviations with periods\n",
        "    text = re.sub(r'([A-Za-z]\\.)+([A-Za-z]\\.)', lambda m: m.group().replace('.', '~DOT~'), text)\n",
        "\n",
        "    # Keep important punctuation for medical text\n",
        "    text = re.sub(r'[^\\w\\s.,;:%\\-\\/()]+', ' ', text)\n",
        "\n",
        "    # Restore preserved abbreviations\n",
        "    text = text.replace('~DOT~', '.')\n",
        "\n",
        "    # Normalize medical measurements\n",
        "    text = re.sub(r'(\\d+)[\\s]*(?:mg|mgs|mcg|µg|ml|mls)', lambda m: f\"{m.group(1)} {m.group()[len(m.group(1)):].strip()}\", text)\n",
        "\n",
        "    # Normalize percentages\n",
        "    text = re.sub(r'(\\d+)[\\s]*(?:percent|pct)', r'\\1%', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# Create enhanced prompts\n",
        "def create_prompt(row):\n",
        "    \"\"\"Create enhanced prompt with medical context\"\"\"\n",
        "    prompt = row['Prompt'].strip()\n",
        "\n",
        "    # Add context if available\n",
        "    context_parts = []\n",
        "    if 'Nursing Competency' in row and not pd.isna(row['Nursing Competency']):\n",
        "        context_parts.append(f\"Competency: {row['Nursing Competency']}\")\n",
        "    if 'Clinical Panel' in row and not pd.isna(row['Clinical Panel']):\n",
        "        context_parts.append(f\"Panel: {row['Clinical Panel']}\")\n",
        "    if 'Years of Experience' in row and not pd.isna(row['Years of Experience']):\n",
        "        context_parts.append(f\"Experience: {int(row['Years of Experience'])} yrs\")\n",
        "\n",
        "    if context_parts:\n",
        "        prompt = f\"Medical Context [{' | '.join(context_parts)}]: {prompt}\"\n",
        "\n",
        "    # Add patient information if found in the text\n",
        "    age_gender = []\n",
        "    age_match = re.search(r'(\\d+)[- ]?(?:year|yr)[- ]old', prompt.lower())\n",
        "    if age_match:\n",
        "        age_gender.append(f\"Age: {age_match.group(1)}\")\n",
        "\n",
        "    gender_match = re.search(r'\\b(male|female|man|woman)\\b', prompt.lower())\n",
        "    if gender_match:\n",
        "        gender = gender_match.group(1).replace(\"man\", \"male\").replace(\"woman\", \"female\")\n",
        "        age_gender.append(f\"Gender: {gender}\")\n",
        "\n",
        "    if age_gender:\n",
        "        prompt = f\"Patient [{ ' | '.join(age_gender) }] - {prompt}\"\n",
        "\n",
        "    return f\"Based on clinical reasoning, provide a concise professional assessment for: {prompt}\"\n",
        "\n",
        "# Apply preprocessing\n",
        "print(\"Preprocessing data...\")\n",
        "train_df['Enhanced_Prompt'] = train_df.apply(create_prompt, axis=1)\n",
        "test_df['Enhanced_Prompt'] = test_df.apply(create_prompt, axis=1)\n",
        "\n",
        "train_df['Enhanced_Prompt'] = train_df['Enhanced_Prompt'].apply(preprocess_text)\n",
        "train_df['Clinician'] = train_df['Clinician'].apply(preprocess_text)\n",
        "test_df['Enhanced_Prompt'] = test_df['Enhanced_Prompt'].apply(preprocess_text)\n",
        "\n",
        "print(\"Sample enhanced prompt:\")\n",
        "print(train_df['Enhanced_Prompt'].iloc[0])\n",
        "print(\"\\nSample clinician response:\")\n",
        "print(train_df['Clinician'].iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7fd32c38",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying enhanced clinical preprocessing...\n",
            "Enhanced preprocessing complete!\n",
            "\n",
            "Sample enhanced clinical reasoning prompt:\n",
            "Medical Case Analysis: Context: Clinical Competency: pediatric emergency burns   Specialty Panel: surgery   Experience Level: Expert (18 years) Patient Details: Age: 4 years (pediatric) Case Presentation: i am a nurse with 18 years of experience in general nursing working in a sub county hospitals and nursing homes in uasin gishu county in kenya a 4 year old child presents to the emergency department with second degree burns on the forearm after accidentally touching a hot stove the child was pl...\n"
          ]
        }
      ],
      "source": [
        "def extract_clinical_demographics(prompt_text):\n",
        "    \"\"\"Extract patient demographics and clinical context from prompt text\"\"\"\n",
        "    demographics = {}\n",
        "    \n",
        "    # Extract age with multiple patterns\n",
        "    age_patterns = [\n",
        "        r'(\\d+)[-\\s]?(?:year|yr)s?[-\\s]?old',\n",
        "        r'age[:\\s]+(\\d+)',\n",
        "        r'(\\d+)[-\\s]?(?:y/o|yo)\\b',\n",
        "    ]\n",
        "    \n",
        "    for pattern in age_patterns:\n",
        "        match = re.search(pattern, prompt_text.lower())\n",
        "        if match:\n",
        "            age = int(match.group(1))\n",
        "            demographics['age'] = age\n",
        "            demographics['age_group'] = (\n",
        "                \"pediatric\" if age < 18 else \n",
        "                \"adult\" if age < 65 else \n",
        "                \"geriatric\"\n",
        "            )\n",
        "            break\n",
        "    \n",
        "    # Extract gender\n",
        "    gender_indicators = {\n",
        "        'male': ['male', 'man', 'gentleman', r'\\bhe\\b', r'\\bhis\\b', r'\\bhim\\b'],\n",
        "        'female': ['female', 'woman', 'lady', r'\\bshe\\b', r'\\bher\\b', r'\\bhers\\b']\n",
        "    }\n",
        "    \n",
        "    for gender, indicators in gender_indicators.items():\n",
        "        for indicator in indicators:\n",
        "            if re.search(indicator, prompt_text.lower()):\n",
        "                demographics['gender'] = gender\n",
        "                break\n",
        "        if 'gender' in demographics:\n",
        "            break\n",
        "    \n",
        "    return demographics\n",
        "\n",
        "def build_clinical_reasoning_prompt(row):\n",
        "    \"\"\"Build sophisticated clinical reasoning prompts with medical context\"\"\"\n",
        "    base_prompt = row['Prompt'].strip()\n",
        "    \n",
        "    # Clinical reasoning templates based on prompt type\n",
        "    reasoning_templates = {\n",
        "        'assessment': \"\"\"Clinical Assessment Request:\n",
        "Context: {context}\n",
        "Patient Information: {patient_info}\n",
        "Clinical Scenario: {base_prompt}\n",
        "\n",
        "Provide a systematic clinical evaluation addressing:\n",
        "1. Primary clinical concerns and key findings\n",
        "2. Differential diagnosis considerations  \n",
        "3. Evidence-based recommendations and interventions\"\"\",\n",
        "        \n",
        "        'case_analysis': \"\"\"Medical Case Analysis:\n",
        "Context: {context}\n",
        "Patient Details: {patient_info}\n",
        "Case Presentation: {base_prompt}\n",
        "\n",
        "Apply clinical reasoning to analyze:\n",
        "• Key clinical features and significance\n",
        "• Potential diagnoses to consider\n",
        "• Appropriate care management plan\"\"\",\n",
        "        \n",
        "        'clinical_decision': \"\"\"Clinical Decision Support:\n",
        "Professional Context: {context}\n",
        "Patient Profile: {patient_info}\n",
        "Clinical Question: {base_prompt}\n",
        "\n",
        "As an experienced clinician, provide professional guidance on:\n",
        "- Clinical assessment and interpretation\n",
        "- Risk factors and considerations\n",
        "- Recommended actions and follow-up\"\"\"\n",
        "    }\n",
        "    \n",
        "    # Determine prompt type\n",
        "    prompt_lower = base_prompt.lower()\n",
        "    if any(word in prompt_lower for word in ['assess', 'evaluate', 'examination']):\n",
        "        template_key = 'assessment'\n",
        "    elif any(word in prompt_lower for word in ['case', 'patient presents', 'scenario']):\n",
        "        template_key = 'case_analysis'\n",
        "    else:\n",
        "        template_key = 'clinical_decision'\n",
        "    \n",
        "    # Build context information\n",
        "    context_parts = []\n",
        "    if 'Nursing Competency' in row and not pd.isna(row['Nursing Competency']):\n",
        "        context_parts.append(f\"Clinical Competency: {row['Nursing Competency']}\")\n",
        "    \n",
        "    if 'Clinical Panel' in row and not pd.isna(row['Clinical Panel']):\n",
        "        context_parts.append(f\"Specialty Panel: {row['Clinical Panel']}\")\n",
        "    \n",
        "    if 'Years of Experience' in row and not pd.isna(row['Years of Experience']):\n",
        "        exp_years = int(row['Years of Experience'])\n",
        "        experience_level = (\n",
        "            \"Novice\" if exp_years < 2 else\n",
        "            \"Competent\" if exp_years < 5 else\n",
        "            \"Proficient\" if exp_years < 10 else\n",
        "            \"Expert\"\n",
        "        )\n",
        "        context_parts.append(f\"Experience Level: {experience_level} ({exp_years} years)\")\n",
        "    \n",
        "    # Extract patient demographics\n",
        "    demographics = extract_clinical_demographics(base_prompt)\n",
        "    patient_info_parts = []\n",
        "    \n",
        "    if 'age' in demographics:\n",
        "        patient_info_parts.append(f\"Age: {demographics['age']} years ({demographics['age_group']})\")\n",
        "    if 'gender' in demographics:\n",
        "        patient_info_parts.append(f\"Gender: {demographics['gender']}\")\n",
        "    \n",
        "    # Format final prompt\n",
        "    context_str = \" | \".join(context_parts) if context_parts else \"General clinical context\"\n",
        "    patient_info_str = \" | \".join(patient_info_parts) if patient_info_parts else \"Patient details not specified\"\n",
        "    \n",
        "    formatted_prompt = reasoning_templates[template_key].format(\n",
        "        context=context_str,\n",
        "        patient_info=patient_info_str,\n",
        "        base_prompt=base_prompt\n",
        "    )\n",
        "    \n",
        "    return formatted_prompt\n",
        "\n",
        "def enhance_clinical_text_preprocessing(text):\n",
        "    \"\"\"Advanced preprocessing specifically for clinical and medical text\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    \n",
        "    # Preserve medical abbreviations and terminology\n",
        "    medical_abbreviations = [\n",
        "        r'([A-Z]{2,}\\.)+',  # Abbreviations like B.P., H.R.\n",
        "        r'\\b[A-Z]+/[A-Z]+\\b',  # Ratios like BP/HR\n",
        "        r'\\b\\d+/\\d+\\b',  # Fractions like 120/80\n",
        "        r'\\b\\d+\\.\\d+\\b',  # Decimals like 98.6\n",
        "    ]\n",
        "    \n",
        "    preserved_terms = {}\n",
        "    for i, pattern in enumerate(medical_abbreviations):\n",
        "        matches = re.findall(pattern, text)\n",
        "        for j, match in enumerate(matches):\n",
        "            placeholder = f\"__MEDICAL_TERM_{i}_{j}__\"\n",
        "            preserved_terms[placeholder] = match\n",
        "            text = text.replace(match, placeholder)\n",
        "    \n",
        "    # Standard text cleaning\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
        "    text = re.sub(r'[^\\w\\s.,;:%\\-\\/()]+', ' ', text)  # Remove special chars but keep medical punctuation\n",
        "    \n",
        "    # Restore preserved medical terms\n",
        "    for placeholder, original in preserved_terms.items():\n",
        "        text = text.replace(placeholder, original)\n",
        "    \n",
        "    # Normalize medical measurements\n",
        "    text = re.sub(r'(\\d+)\\s*(mg|mgs|mcg|µg|ml|mls|kg|lbs?)', r'\\1 \\2', text)\n",
        "    text = re.sub(r'(\\d+)\\s*(?:percent|pct|%)', r'\\1%', text)\n",
        "    \n",
        "    return text.strip()\n",
        "\n",
        "# Apply enhanced preprocessing\n",
        "print(\"Applying enhanced clinical preprocessing...\")\n",
        "train_df['Clinical_Reasoning_Prompt'] = train_df.apply(build_clinical_reasoning_prompt, axis=1)\n",
        "test_df['Clinical_Reasoning_Prompt'] = test_df.apply(build_clinical_reasoning_prompt, axis=1)\n",
        "\n",
        "train_df['Clinical_Reasoning_Prompt'] = train_df['Clinical_Reasoning_Prompt'].apply(enhance_clinical_text_preprocessing)\n",
        "train_df['Clinician'] = train_df['Clinician'].apply(enhance_clinical_text_preprocessing)\n",
        "test_df['Clinical_Reasoning_Prompt'] = test_df['Clinical_Reasoning_Prompt'].apply(enhance_clinical_text_preprocessing)\n",
        "\n",
        "print(\"Enhanced preprocessing complete!\")\n",
        "print(\"\\nSample enhanced clinical reasoning prompt:\")\n",
        "print(train_df['Clinical_Reasoning_Prompt'].iloc[0][:500] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75c7b6b1",
      "metadata": {
        "id": "75c7b6b1"
      },
      "source": [
        "## Data Analysis and Visualization\n",
        "\n",
        "Analyze the dataset and visualize key characteristics to gain insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "37c39bd3",
      "metadata": {
        "id": "37c39bd3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt length statistics:\n",
            "Mean: 728.3, Median: 711.5\n",
            "Min: 374, Max: 1562\n",
            "\n",
            "Clinician response length statistics:\n",
            "Mean: 695.3, Median: 650.0\n",
            "Min: 151, Max: 2047\n",
            "\n",
            "95.0% of prompts are over 500 characters\n",
            "98.8% of responses are over 200 characters\n"
          ]
        }
      ],
      "source": [
        "# Analyze data distributions\n",
        "def analyze_data(df):\n",
        "    \"\"\"Analyze and visualize data characteristics\"\"\"\n",
        "    # Calculate text lengths\n",
        "    df['prompt_length'] = df['Enhanced_Prompt'].str.len()\n",
        "    if 'Clinician' in df.columns:\n",
        "        df['clinician_length'] = df['Clinician'].str.len()\n",
        "\n",
        "    # Plot distributions\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.histplot(df['prompt_length'], bins=30, kde=True)\n",
        "    plt.title('Distribution of Prompt Lengths')\n",
        "    plt.xlabel('Character Count')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    if 'Clinician' in df.columns:\n",
        "        plt.subplot(1, 2, 2)\n",
        "        sns.histplot(df['clinician_length'], bins=30, kde=True)\n",
        "        plt.title('Distribution of Clinician Response Lengths')\n",
        "        plt.xlabel('Character Count')\n",
        "        plt.ylabel('Frequency')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print statistics\n",
        "    print(f\"Prompt length statistics:\")\n",
        "    print(f\"Mean: {df['prompt_length'].mean():.1f}, Median: {df['prompt_length'].median():.1f}\")\n",
        "    print(f\"Min: {df['prompt_length'].min()}, Max: {df['prompt_length'].max()}\")\n",
        "\n",
        "    if 'Clinician' in df.columns:\n",
        "        print(f\"\\nClinician response length statistics:\")\n",
        "        print(f\"Mean: {df['clinician_length'].mean():.1f}, Median: {df['clinician_length'].median():.1f}\")\n",
        "        print(f\"Min: {df['clinician_length'].min()}, Max: {df['clinician_length'].max()}\")\n",
        "\n",
        "        # Calculate additional statistics\n",
        "        prompt_over_500 = (df['prompt_length'] > 500).mean() * 100\n",
        "        response_over_200 = (df['clinician_length'] > 200).mean() * 100\n",
        "        print(f\"\\n{prompt_over_500:.1f}% of prompts are over 500 characters\")\n",
        "        print(f\"{response_over_200:.1f}% of responses are over 200 characters\")\n",
        "\n",
        "# Run analysis\n",
        "analyze_data(train_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79f537b8",
      "metadata": {
        "id": "79f537b8"
      },
      "source": [
        "## Model Setup\n",
        "\n",
        "Initialize the T5 model and tokenizer for sequence-to-sequence training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fbaa9aa8",
      "metadata": {
        "id": "fbaa9aa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading google/flan-t5-large model and tokenizer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mirac\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded with 783,150,080 parameters\n"
          ]
        }
      ],
      "source": [
        "# Define model parameters\n",
        "MODEL_NAME = \"google/flan-t5-large\"  # Options: t5-small, t5-base, t5-large\n",
        "MAX_SOURCE_LENGTH = 512\n",
        "MAX_TARGET_LENGTH = 150\n",
        "BATCH_SIZE = 4\n",
        "GRADIENT_ACCUMULATION_STEPS = 8\n",
        "\n",
        "# Load tokenizer and model\n",
        "print(f\"Loading {MODEL_NAME} model and tokenizer...\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "print(f\"Model loaded with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "\n",
        "# Enable gradient checkpointing for memory efficiency\n",
        "model.gradient_checkpointing_enable()\n",
        "model.config.use_cache = False  # Disable KV cache during training\n",
        "\n",
        "# Tokenize data efficiently\n",
        "def tokenize_clinical_data(examples, max_source_length=MAX_SOURCE_LENGTH, max_target_length=MAX_TARGET_LENGTH):\n",
        "    \"\"\"Tokenize inputs and targets with proper T5 decoder setup\"\"\"\n",
        "    \n",
        "    # Tokenize inputs\n",
        "    model_inputs = tokenizer(\n",
        "        [\"clinical reasoning: \" + text for text in examples['Clinical_Reasoning_Prompt']],\n",
        "        max_length=max_source_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors=None\n",
        "    )\n",
        "\n",
        "    # Tokenize targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            examples['Clinician'],\n",
        "            max_length=max_target_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=None\n",
        "        )\n",
        "\n",
        "    # Replace padding tokens with -100 for loss calculation\n",
        "    labels[\"input_ids\"] = [\n",
        "        [(l if l != tokenizer.pad_token_id else -100) for l in label] \n",
        "        for label in labels[\"input_ids\"]\n",
        "    ]\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "71315a3a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Data collator created for T5\n"
          ]
        }
      ],
      "source": [
        "# Add Data Collector\n",
        "\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "# Create T5-specific data collator\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True,\n",
        "    max_length=MAX_SOURCE_LENGTH,\n",
        "    label_pad_token_id=-100,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(\"✅ Data collator created for T5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b0e5a7db",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧹 Performing complete memory reset...\n",
            "✅ Deleted large objects\n",
            "🗑️ Garbage collection pass 1\n",
            "🗑️ Garbage collection pass 2\n",
            "🗑️ Garbage collection pass 3\n",
            "🗑️ Garbage collection pass 4\n",
            "🗑️ Garbage collection pass 5\n",
            "⚠️ CUDA cleanup error: CUDA error: out of memory\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "Continuing with CPU fallback...\n",
            "🔄 Reloading minimal components...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mirac\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️ Loading model on CPU due to memory constraints\n",
            "❌ Model loading failed: CUDA error: out of memory\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "💡 Your GPU memory is completely exhausted. Consider restarting the kernel.\n",
            "\n",
            "🎯 Memory reset complete! Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# === COMPLETE MEMORY RESET ===\n",
        "print(\"🧹 Performing complete memory reset...\")\n",
        "\n",
        "# Step 1: Delete all large objects\n",
        "try:\n",
        "    if 'model' in globals():\n",
        "        del model\n",
        "    if 'tokenizer' in globals():\n",
        "        # Don't delete tokenizer completely, we need it\n",
        "        pass\n",
        "    if 'tokenized_train' in globals():\n",
        "        del tokenized_train\n",
        "    if 'tokenized_val' in globals():\n",
        "        del tokenized_val\n",
        "    if 'train_df' in globals():\n",
        "        # Keep but reduce memory\n",
        "        train_df = train_df[['Clinical_Reasoning_Prompt', 'Clinician']].copy()\n",
        "    print(\"✅ Deleted large objects\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Step 2: Force garbage collection multiple times\n",
        "import gc\n",
        "for i in range(5):  # Multiple passes\n",
        "    gc.collect()\n",
        "    print(f\"🗑️ Garbage collection pass {i+1}\")\n",
        "\n",
        "# Step 3: CUDA memory management with error handling\n",
        "if torch.cuda.is_available():\n",
        "    try:\n",
        "        # Force synchronization\n",
        "        torch.cuda.synchronize()\n",
        "        print(\"✅ CUDA synchronized\")\n",
        "        \n",
        "        # Get memory info before cleanup\n",
        "        allocated_before = torch.cuda.memory_allocated() / (1024**3)\n",
        "        print(f\"Memory allocated before cleanup: {allocated_before:.2f} GB\")\n",
        "        \n",
        "        # Clear cache with error handling\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"✅ CUDA cache cleared\")\n",
        "        \n",
        "        # Get memory info after cleanup\n",
        "        allocated_after = torch.cuda.memory_allocated() / (1024**3)\n",
        "        print(f\"Memory allocated after cleanup: {allocated_after:.2f} GB\")\n",
        "        \n",
        "    except RuntimeError as e:\n",
        "        print(f\"⚠️ CUDA cleanup error: {e}\")\n",
        "        print(\"Continuing with CPU fallback...\")\n",
        "        # Force CPU mode\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "# Step 4: Reload minimal components for baseline\n",
        "print(\"🔄 Reloading minimal components...\")\n",
        "\n",
        "# Reload tokenizer only\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\", use_fast=True)\n",
        "\n",
        "# Reload model with minimal memory footprint\n",
        "try:\n",
        "    if torch.cuda.is_available() and torch.cuda.memory_allocated() / (1024**3) < 10:  # Less than 10GB used\n",
        "        model = T5ForConditionalGeneration.from_pretrained(\n",
        "            \"google/flan-t5-large\",\n",
        "            torch_dtype=torch.float16,  # Use FP16 to save memory\n",
        "            low_cpu_mem_usage=True\n",
        "        ).to(device)\n",
        "        print(\"✅ Model loaded on GPU with FP16\")\n",
        "    else:\n",
        "        print(\"⚠️ Loading model on CPU due to memory constraints\")\n",
        "        device = torch.device(\"cpu\")\n",
        "        model = T5ForConditionalGeneration.from_pretrained(\n",
        "            \"google/flan-t5-large\",\n",
        "            torch_dtype=torch.float32,\n",
        "            low_cpu_mem_usage=True\n",
        "        ).to(device)\n",
        "        print(\"✅ Model loaded on CPU\")\n",
        "        \n",
        "    model.eval()\n",
        "    print(f\"✅ Model ready on {device}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Model loading failed: {e}\")\n",
        "    print(\"💡 Your GPU memory is completely exhausted. Consider restarting the kernel.\")\n",
        "    \n",
        "print(f\"\\n🎯 Memory reset complete! Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9445c5bf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Creating baseline submission using pre-trained FLAN-T5...\n",
            "This will use the sophisticated clinical prompts created in preprocessing.\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 88\u001b[0m\n\u001b[0;32m     86\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m---> 88\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Generate the submission\u001b[39;00m\n\u001b[0;32m     91\u001b[0m baseline_submission \u001b[38;5;241m=\u001b[39m create_baseline_submission()\n",
            "File \u001b[1;32mc:\\Users\\mirac\\anaconda3\\Lib\\site-packages\\torch\\cuda\\memory.py:222\u001b[0m, in \u001b[0;36mempty_cache\u001b[1;34m()\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[1;32m--> 222\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_emptyCache()\n",
            "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "# === BASELINE SUBMISSION USING PRE-TRAINED MODEL ===\n",
        "# This creates a working submission without training\n",
        "\n",
        "def create_baseline_submission():\n",
        "    \"\"\"Create submission using the already loaded FLAN-T5 model without training\"\"\"\n",
        "    print(\"🔄 Creating baseline submission with pre-trained FLAN-T5...\")\n",
        "    \n",
        "    # Use the model and tokenizer already loaded in previous cells\n",
        "    model.eval()\n",
        "    \n",
        "    predictions = []\n",
        "    batch_size = 8\n",
        "    \n",
        "    print(f\"Processing {len(test_df)} test samples...\")\n",
        "    \n",
        "    # Use the Clinical_Reasoning_Prompt column created in preprocessing\n",
        "    test_prompts = test_df['Clinical_Reasoning_Prompt'].tolist()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(test_prompts), batch_size):\n",
        "            if i % 50 == 0:\n",
        "                print(f\"Progress: {i}/{len(test_prompts)}\")\n",
        "            \n",
        "            # Get batch of prompts\n",
        "            batch_prompts = test_prompts[i:i+batch_size]\n",
        "            \n",
        "            # Add clinical reasoning prefix\n",
        "            formatted_prompts = [\"clinical reasoning: \" + prompt for prompt in batch_prompts]\n",
        "            \n",
        "            # Tokenize\n",
        "            inputs = tokenizer(\n",
        "                formatted_prompts,\n",
        "                max_length=MAX_SOURCE_LENGTH,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(device)\n",
        "            \n",
        "            # Generate predictions\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_length=MAX_TARGET_LENGTH,\n",
        "                num_beams=2,\n",
        "                early_stopping=True,\n",
        "                do_sample=False,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "            \n",
        "            # Decode predictions\n",
        "            batch_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "            predictions.extend(batch_preds)\n",
        "    \n",
        "    # Create submission DataFrame\n",
        "    submission_df = pd.DataFrame({\n",
        "        'ID': test_df['Master_Index'],\n",
        "        'Clinician': predictions\n",
        "    })\n",
        "    \n",
        "    # Save submission\n",
        "    submission_path = 'baseline_submission.csv'\n",
        "    submission_df.to_csv(submission_path, index=False)\n",
        "    print(f\"✅ Baseline submission saved to '{submission_path}'\")\n",
        "    \n",
        "    # Show sample predictions\n",
        "    print(\"\\n📋 Sample predictions:\")\n",
        "    for i in range(min(3, len(predictions))):\n",
        "        print(f\"\\nSample {i+1}:\")\n",
        "        print(f\"Input: {test_df['Clinical_Reasoning_Prompt'].iloc[i][:100]}...\")\n",
        "        print(f\"Prediction: {predictions[i]}\")\n",
        "        print(\"-\" * 80)\n",
        "    \n",
        "    # Show submission statistics\n",
        "    print(f\"\\n📊 Submission Statistics:\")\n",
        "    print(f\"Total predictions: {len(predictions)}\")\n",
        "    print(f\"Average prediction length: {np.mean([len(pred.split()) for pred in predictions]):.1f} words\")\n",
        "    print(f\"Prediction length range: {min([len(pred.split()) for pred in predictions])}-{max([len(pred.split()) for pred in predictions])} words\")\n",
        "    \n",
        "    return submission_df\n",
        "\n",
        "# Execute baseline submission creation\n",
        "print(\"🚀 Creating baseline submission using pre-trained FLAN-T5...\")\n",
        "print(\"This will use the sophisticated clinical prompts created in preprocessing.\")\n",
        "\n",
        "# Clear GPU memory before inference\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Generate the submission\n",
        "baseline_submission = create_baseline_submission()\n",
        "\n",
        "print(\"\\n🎯 Baseline submission complete!\")\n",
        "print(\"You can now submit the 'baseline_submission.csv' file.\")\n",
        "print(\"This uses the pre-trained FLAN-T5 model with your enhanced clinical prompts.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e39b6a98",
      "metadata": {
        "id": "e39b6a98"
      },
      "source": [
        "## Prepare Datasets\n",
        "\n",
        "Prepare and split the dataset for training and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b2429533",
      "metadata": {
        "id": "b2429533"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stratification groups: 8\n",
            "Smallest group size: 1\n",
            "Groups with <2 samples: 1\n",
            "⚠️ Some groups too small, using simpler stratification...\n",
            "✅ Using response length stratification: {'short': 134, 'long': 134, 'medium': 132}\n",
            "✅ Split successful!\n",
            "Training set: 320 examples\n",
            "Validation set: 80 examples\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cfff410361ba490d8bbe18af289eba22",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing clinical training data:   0%|          | 0/320 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mirac\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85e0d04d92c94a828e37924d39f988b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing clinical validation data:   0%|          | 0/80 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2d26e303c98468c85fb1a1933537060",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing clinical test data:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create datasets for training\n",
        "def prepare_clinical_datasets(df, test_size=0.2):\n",
        "    \"\"\"Prepare datasets for clinical reasoning training with robust stratification\"\"\"\n",
        "    \n",
        "    # Create more robust stratification categories\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Create broader categories to ensure sufficient samples per group\n",
        "    df['response_length_category'] = pd.qcut(\n",
        "        df['Clinician'].str.len(), \n",
        "        q=3,  # Reduced from 4 to 3 for more samples per group\n",
        "        labels=['short', 'medium', 'long'],\n",
        "        duplicates='drop'  # Handle duplicate bin edges\n",
        "    )\n",
        "    \n",
        "    # Create simpler prompt complexity categories\n",
        "    prompt_lengths = df['Clinical_Reasoning_Prompt'].str.len()\n",
        "    df['prompt_complexity'] = pd.cut(\n",
        "        prompt_lengths,\n",
        "        bins=3,  # Use 3 bins instead of length-based division\n",
        "        labels=['simple', 'moderate', 'complex'],\n",
        "        duplicates='drop'\n",
        "    )\n",
        "    \n",
        "    # Create stratification key\n",
        "    df['stratification_key'] = (\n",
        "        df['response_length_category'].astype(str) + '_' + \n",
        "        df['prompt_complexity'].astype(str)\n",
        "    )\n",
        "    \n",
        "    # Check group sizes before stratification\n",
        "    group_counts = df['stratification_key'].value_counts()\n",
        "    print(f\"Stratification groups: {len(group_counts)}\")\n",
        "    print(f\"Smallest group size: {group_counts.min()}\")\n",
        "    print(f\"Groups with <2 samples: {(group_counts < 2).sum()}\")\n",
        "    \n",
        "    # If any group has <2 samples, fall back to simpler stratification\n",
        "    if group_counts.min() < 2:\n",
        "        print(\"⚠️ Some groups too small, using simpler stratification...\")\n",
        "        # Use only response length for stratification\n",
        "        stratify_column = df['response_length_category']\n",
        "        \n",
        "        # Check if even this is viable\n",
        "        simple_counts = stratify_column.value_counts()\n",
        "        if simple_counts.min() < 2:\n",
        "            print(\"⚠️ Even simpler stratification has small groups, using random split...\")\n",
        "            stratify_column = None\n",
        "        else:\n",
        "            print(f\"✅ Using response length stratification: {simple_counts.to_dict()}\")\n",
        "    else:\n",
        "        print(\"✅ Using combined stratification\")\n",
        "        stratify_column = df['stratification_key']\n",
        "    \n",
        "    # Perform the split\n",
        "    try:\n",
        "        train_df, val_df = train_test_split(\n",
        "            df,\n",
        "            test_size=test_size,\n",
        "            random_state=SEED,\n",
        "            stratify=stratify_column\n",
        "        )\n",
        "        print(f\"✅ Split successful!\")\n",
        "    except ValueError as e:\n",
        "        print(f\"❌ Stratified split failed: {e}\")\n",
        "        print(\"🔄 Falling back to random split...\")\n",
        "        train_df, val_df = train_test_split(\n",
        "            df,\n",
        "            test_size=test_size,\n",
        "            random_state=SEED,\n",
        "            stratify=None  # No stratification\n",
        "        )\n",
        "        print(\"✅ Random split completed\")\n",
        "\n",
        "    print(f\"Training set: {len(train_df)} examples\")\n",
        "    print(f\"Validation set: {len(val_df)} examples\")\n",
        "\n",
        "    # Convert to datasets format using clinical prompts\n",
        "    train_dataset = Dataset.from_pandas(train_df[['Clinical_Reasoning_Prompt', 'Clinician']])\n",
        "    val_dataset = Dataset.from_pandas(val_df[['Clinical_Reasoning_Prompt', 'Clinician']])\n",
        "\n",
        "    # Apply tokenization\n",
        "    tokenized_train = train_dataset.map(\n",
        "        tokenize_clinical_data,\n",
        "        batched=True,\n",
        "        batch_size=16,\n",
        "        remove_columns=['Clinical_Reasoning_Prompt', 'Clinician'],\n",
        "        desc=\"Tokenizing clinical training data\"\n",
        "    )\n",
        "\n",
        "    tokenized_val = val_dataset.map(\n",
        "        tokenize_clinical_data,\n",
        "        batched=True,\n",
        "        batch_size=16,\n",
        "        remove_columns=['Clinical_Reasoning_Prompt', 'Clinician'],\n",
        "        desc=\"Tokenizing clinical validation data\"\n",
        "    )\n",
        "\n",
        "    return tokenized_train, tokenized_val, train_df, val_df\n",
        "\n",
        "def prepare_clinical_test_dataset(test_df):\n",
        "    \"\"\"Prepare test dataset for clinical reasoning inference\"\"\"\n",
        "    test_dataset = Dataset.from_pandas(test_df[['Clinical_Reasoning_Prompt']])\n",
        "\n",
        "    # Tokenize test data with clinical reasoning prefix\n",
        "    tokenized_test = test_dataset.map(\n",
        "        lambda examples: tokenizer(\n",
        "            [\"clinical reasoning: \" + text for text in examples['Clinical_Reasoning_Prompt']],\n",
        "            max_length=MAX_SOURCE_LENGTH,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ),\n",
        "        batched=True,\n",
        "        batch_size=16,\n",
        "        remove_columns=['Clinical_Reasoning_Prompt'],\n",
        "        desc=\"Tokenizing clinical test data\"\n",
        "    )\n",
        "\n",
        "    return tokenized_test\n",
        "\n",
        "# Prepare datasets with enhanced clinical processing\n",
        "tokenized_train, tokenized_val, train_subset, val_subset = prepare_clinical_datasets(train_df)\n",
        "tokenized_test = prepare_clinical_test_dataset(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32ec283e",
      "metadata": {
        "id": "32ec283e"
      },
      "source": [
        "## Evaluation Metrics\n",
        "\n",
        "Define evaluation metrics including ROUGE scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "242aadcc",
      "metadata": {
        "id": "242aadcc"
      },
      "outputs": [],
      "source": [
        "# Evaluation metrics function\n",
        "def compute_clinical_reasoning_metrics(evaluation_predictions):\n",
        "    \"\"\"Comprehensive evaluation metrics for clinical reasoning tasks\"\"\"\n",
        "    predictions, labels = evaluation_predictions\n",
        "    \n",
        "    # Handle tuple predictions (when using predict_with_generate=True)\n",
        "    if isinstance(predictions, tuple):\n",
        "        predictions = predictions[0]\n",
        "    \n",
        "    # Decode predictions and labels safely\n",
        "    try:\n",
        "        decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    except:\n",
        "        # Fallback for problematic sequences\n",
        "        decoded_predictions = []\n",
        "        for pred_seq in predictions:\n",
        "            try:\n",
        "                decoded = tokenizer.decode(pred_seq, skip_special_tokens=True)\n",
        "                decoded_predictions.append(decoded)\n",
        "            except:\n",
        "                decoded_predictions.append(\"\")\n",
        "    \n",
        "    # Handle labels (replace -100 with pad token)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    try:\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    except:\n",
        "        decoded_labels = []\n",
        "        for label_seq in labels:\n",
        "            try:\n",
        "                decoded = tokenizer.decode(label_seq, skip_special_tokens=True)\n",
        "                decoded_labels.append(decoded)\n",
        "            except:\n",
        "                decoded_labels.append(\"\")\n",
        "    \n",
        "    # Clean up decoded text\n",
        "    decoded_predictions = [pred.strip() for pred in decoded_predictions]\n",
        "    decoded_labels = [label.strip() for label in decoded_labels]\n",
        "    \n",
        "    # Standard ROUGE metrics\n",
        "    rouge_metrics = calculate_rouge_scores_safe(decoded_predictions, decoded_labels)\n",
        "    \n",
        "    # Clinical-specific metrics\n",
        "    clinical_metrics = evaluate_clinical_reasoning_quality(decoded_predictions, decoded_labels)\n",
        "    \n",
        "    # Combine all metrics\n",
        "    all_metrics = {**rouge_metrics, **clinical_metrics}\n",
        "    \n",
        "    # Add prefix for trainer compatibility\n",
        "    prefixed_metrics = {}\n",
        "    for key, value in all_metrics.items():\n",
        "        if not key.startswith('eval_'):\n",
        "            prefixed_metrics[f'eval_{key}'] = value\n",
        "        else:\n",
        "            prefixed_metrics[key] = value\n",
        "    \n",
        "    return prefixed_metrics\n",
        "\n",
        "def calculate_rouge_scores_safe(predictions, references):\n",
        "    \"\"\"Calculate ROUGE scores with error handling\"\"\"\n",
        "    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    \n",
        "    rouge1_scores = []\n",
        "    rouge2_scores = []\n",
        "    rougeL_scores = []\n",
        "    \n",
        "    for pred, ref in zip(predictions, references):\n",
        "        if not pred.strip() or not ref.strip():\n",
        "            rouge1_scores.append(0.0)\n",
        "            rouge2_scores.append(0.0)\n",
        "            rougeL_scores.append(0.0)\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            scores = rouge_scorer_instance.score(ref, pred)\n",
        "            rouge1_scores.append(scores['rouge1'].fmeasure)\n",
        "            rouge2_scores.append(scores['rouge2'].fmeasure)\n",
        "            rougeL_scores.append(scores['rougeL'].fmeasure)\n",
        "        except:\n",
        "            rouge1_scores.append(0.0)\n",
        "            rouge2_scores.append(0.0)\n",
        "            rougeL_scores.append(0.0)\n",
        "    \n",
        "    return {\n",
        "        'rouge1': np.mean(rouge1_scores),\n",
        "        'rouge2': np.mean(rouge2_scores),\n",
        "        'rougeL': np.mean(rougeL_scores),\n",
        "        'clinical_rouge': np.mean(rougeL_scores)  # Primary metric for clinical reasoning\n",
        "    }\n",
        "\n",
        "def evaluate_clinical_reasoning_quality(predictions, references):\n",
        "    \"\"\"Evaluate clinical reasoning specific quality metrics\"\"\"\n",
        "    \n",
        "    # Clinical terminology that should appear in good responses\n",
        "    clinical_keywords = [\n",
        "        'assess', 'evaluate', 'diagnosis', 'treatment', 'intervention',\n",
        "        'patient', 'clinical', 'medical', 'care', 'management', 'monitor',\n",
        "        'recommend', 'consider', 'follow-up', 'plan', 'history', 'examination'\n",
        "    ]\n",
        "    \n",
        "    # Calculate clinical terminology coverage\n",
        "    clinical_coverage_scores = []\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        pred_lower = pred.lower()\n",
        "        ref_lower = ref.lower()\n",
        "        \n",
        "        # Count clinical terms in prediction and reference\n",
        "        pred_clinical_terms = sum(1 for term in clinical_keywords if term in pred_lower)\n",
        "        ref_clinical_terms = sum(1 for term in clinical_keywords if term in ref_lower)\n",
        "        \n",
        "        if ref_clinical_terms > 0:\n",
        "            coverage = min(pred_clinical_terms / ref_clinical_terms, 1.0)\n",
        "        else:\n",
        "            coverage = 1.0 if pred_clinical_terms == 0 else 0.5\n",
        "        \n",
        "        clinical_coverage_scores.append(coverage)\n",
        "    \n",
        "    # Evaluate response structure and organization\n",
        "    structure_scores = []\n",
        "    for pred in predictions:\n",
        "        score = 0.0\n",
        "        \n",
        "        # Check for organized structure (lists, numbering, bullet points)\n",
        "        if any(marker in pred for marker in ['1.', '2.', '3.', '•', '-', ':']):\n",
        "            score += 0.3\n",
        "        \n",
        "        # Check for appropriate length (not too short or too long)\n",
        "        word_count = len(pred.split())\n",
        "        if 15 <= word_count <= 100:\n",
        "            score += 0.4\n",
        "        elif 10 <= word_count <= 150:\n",
        "            score += 0.2\n",
        "        \n",
        "        # Check for clinical reasoning structure\n",
        "        reasoning_indicators = ['because', 'due to', 'therefore', 'as a result', 'indicates', 'suggests']\n",
        "        if any(indicator in pred.lower() for indicator in reasoning_indicators):\n",
        "            score += 0.3\n",
        "        \n",
        "        structure_scores.append(score)\n",
        "    \n",
        "    # Calculate response completeness\n",
        "    completeness_scores = []\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        pred_words = set(pred.lower().split())\n",
        "        ref_words = set(ref.lower().split())\n",
        "        \n",
        "        if len(ref_words) > 0:\n",
        "            overlap = len(pred_words.intersection(ref_words)) / len(ref_words)\n",
        "            completeness_scores.append(min(overlap, 1.0))\n",
        "        else:\n",
        "            completeness_scores.append(0.0)\n",
        "    \n",
        "    return {\n",
        "        'clinical_terminology_coverage': np.mean(clinical_coverage_scores),\n",
        "        'response_structure_quality': np.mean(structure_scores),\n",
        "        'response_completeness': np.mean(completeness_scores),\n",
        "        'average_response_length': np.mean([len(pred.split()) for pred in predictions])\n",
        "    }\n",
        "\n",
        "# Update the compute_metrics function to use clinical metrics\n",
        "compute_metrics = compute_clinical_reasoning_metrics\n",
        "\n",
        "# Custom prediction generation function\n",
        "def generate_predictions(model, dataset, tokenizer, batch_size=8, max_length=128):\n",
        "    \"\"\"Generate predictions from the model for the dataset\"\"\"\n",
        "    predictions = []\n",
        "    model.eval()\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Generating predictions\"):\n",
        "            # Move batch to device\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            # Generate predictions\n",
        "            outputs = model.generate(\n",
        "                input_ids=batch['input_ids'],\n",
        "                attention_mask=batch['attention_mask'],\n",
        "                max_length=max_length,\n",
        "                num_beams=4,\n",
        "                length_penalty=1.0,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "            # Decode predictions\n",
        "            decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "            predictions.extend(decoded_outputs)\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5a9f568",
      "metadata": {
        "id": "c5a9f568"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "Set up the training arguments and train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "290efc16",
      "metadata": {
        "id": "290efc16"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 28\u001b[0m\n\u001b[0;32m      2\u001b[0m training_args \u001b[38;5;241m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[0;32m      3\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./clinical_model_optimized\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     overwrite_output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     dataloader_num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,         \u001b[38;5;66;03m# Changed from 2 to avoid hanging\u001b[39;00m\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Initialize trainer\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m     29\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     30\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     31\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_train,\n\u001b[0;32m     32\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_val,\n\u001b[0;32m     33\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     34\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\mirac\\anaconda3\\Lib\\site-packages\\transformers\\trainer_seq2seq.py:56\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     44\u001b[0m     model: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedModel\u001b[39m\u001b[38;5;124m\"\u001b[39m, nn\u001b[38;5;241m.\u001b[39mModule] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m     preprocess_logits_for_metrics: Optional[Callable[[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     55\u001b[0m ):\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     57\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     58\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m     59\u001b[0m         data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m     60\u001b[0m         train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m     61\u001b[0m         eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset,\n\u001b[0;32m     62\u001b[0m         tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m     63\u001b[0m         model_init\u001b[38;5;241m=\u001b[39mmodel_init,\n\u001b[0;32m     64\u001b[0m         compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     65\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m     66\u001b[0m         optimizers\u001b[38;5;241m=\u001b[39moptimizers,\n\u001b[0;32m     67\u001b[0m         preprocess_logits_for_metrics\u001b[38;5;241m=\u001b[39mpreprocess_logits_for_metrics,\n\u001b[0;32m     68\u001b[0m     )\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# Override self.model.generation_config if a GenerationConfig is specified in args.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# Priority: args.generation_config > model.generation_config > default GenerationConfig.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\mirac\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:337\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n\u001b[0;32m    336\u001b[0m \u001b[38;5;66;03m# Seed must be set before instantiating the model when using model\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m enable_full_determinism(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mseed) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mfull_determinism \u001b[38;5;28;01melse\u001b[39;00m set_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\mirac\\anaconda3\\Lib\\site-packages\\transformers\\trainer_utils.py:95\u001b[0m, in \u001b[0;36mset_seed\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m     93\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[1;32m---> 95\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m     96\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed_all(seed)\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# ^^ safe to call this function even if cuda is not available\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\mirac\\anaconda3\\Lib\\site-packages\\torch\\_compile.py:51\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     48\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[0;32m     49\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m disable_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\mirac\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    836\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    839\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    840\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\mirac\\anaconda3\\Lib\\site-packages\\torch\\random.py:46\u001b[0m, in \u001b[0;36mmanual_seed\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n\u001b[1;32m---> 46\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed_all(seed)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmps\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n",
            "File \u001b[1;32mc:\\Users\\mirac\\anaconda3\\Lib\\site-packages\\torch\\cuda\\random.py:128\u001b[0m, in \u001b[0;36mmanual_seed_all\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m    125\u001b[0m         default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[0;32m    126\u001b[0m         default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m--> 128\u001b[0m _lazy_call(cb, seed_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\mirac\\anaconda3\\Lib\\site-packages\\torch\\cuda\\__init__.py:302\u001b[0m, in \u001b[0;36m_lazy_call\u001b[1;34m(callable, **kwargs)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _initialization_lock:\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[1;32m--> 302\u001b[0m         \u001b[38;5;28mcallable\u001b[39m()\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n",
            "File \u001b[1;32mc:\\Users\\mirac\\anaconda3\\Lib\\site-packages\\torch\\cuda\\random.py:126\u001b[0m, in \u001b[0;36mmanual_seed_all.<locals>.cb\u001b[1;34m()\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[0;32m    125\u001b[0m     default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[1;32m--> 126\u001b[0m     default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "# Enhanced training arguments for clinical reasoning\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./clinical_model_optimized\",\n",
        "    overwrite_output_dir=True,\n",
        "    evaluation_strategy=\"epoch\",  # Changed from \"steps\" \n",
        "    save_strategy=\"epoch\",        # Changed from \"steps\"\n",
        "    learning_rate=3e-4,           # Increased from 5e-5\n",
        "    weight_decay=0.01,\n",
        "    per_device_train_batch_size=2,    # Reduced from 4\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=16,   # Increased from 8\n",
        "    num_train_epochs=3,               # Reduced from 8\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=128,        # Reduced from 150\n",
        "    save_total_limit=2,               # Reduced from 3\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_rougeL\",  # Changed from \"eval_clinical_rouge\"\n",
        "    greater_is_better=True,\n",
        "    warmup_steps=50,                  # Reduced from 100\n",
        "    logging_steps=20,                 # Reduced from 25\n",
        "    report_to=\"none\",\n",
        "    label_smoothing_factor=0.1,\n",
        "    dataloader_num_workers=0,         # Changed from 2 to avoid hanging\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "print(\"Starting training...\")\n",
        "train_start_time = time.time()\n",
        "\n",
        "# Clear memory before training\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "try:\n",
        "    trainer.train()\n",
        "    train_duration = time.time() - train_start_time\n",
        "    print(f\"✅ Training completed in {train_duration / 60:.2f} minutes\")\n",
        "    \n",
        "    # Evaluate model\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(\"\\nEvaluation results:\")\n",
        "    for key, value in eval_results.items():\n",
        "        print(f\"{key}: {value:.4f}\")\n",
        "    \n",
        "    # Save the model\n",
        "    model_save_path = \"./clinical_model_final\"\n",
        "    trainer.save_model(model_save_path)\n",
        "    tokenizer.save_pretrained(model_save_path)\n",
        "    print(f\"Model saved to {model_save_path}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Training failed: {e}\")\n",
        "    print(\"Trying with even smaller batch size...\")\n",
        "    \n",
        "    # Fallback with smaller batch\n",
        "    training_args.per_device_train_batch_size = 1\n",
        "    training_args.gradient_accumulation_steps = 32\n",
        "    \n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train,\n",
        "        eval_dataset=tokenized_val,\n",
        "        compute_metrics=compute_metrics,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "    \n",
        "    trainer.train()\n",
        "    print(\"✅ Fallback training completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d949e4c8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformers version: 4.35.2\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "print(f\"Transformers version: {transformers.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e20691a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_clinical_reasoning_model_with_cross_validation(train_data, model_name=\"google/flan-t5-large\", n_folds=3):\n",
        "    \"\"\"Train models using cross-validation for robust performance\"\"\"\n",
        "    from sklearn.model_selection import StratifiedKFold\n",
        "    from transformers import EarlyStoppingCallback\n",
        "    \n",
        "    # Create stratification key based on response characteristics\n",
        "    train_data = train_data.copy()\n",
        "    \n",
        "    try:\n",
        "        train_data['response_length_category'] = pd.qcut(\n",
        "            train_data['Clinician'].str.len(), \n",
        "            q=3,  # Reduced to 3 for better sample distribution\n",
        "            labels=['short', 'medium', 'long'],\n",
        "            duplicates='drop'\n",
        "        )\n",
        "    except:\n",
        "        # Fallback if qcut fails\n",
        "        train_data['response_length_category'] = pd.cut(\n",
        "            train_data['Clinician'].str.len(), \n",
        "            bins=3,\n",
        "            labels=['short', 'medium', 'long']\n",
        "        )\n",
        "    \n",
        "    # Add complexity category based on prompt characteristics\n",
        "    train_data['prompt_complexity'] = (train_data['Clinical_Reasoning_Prompt'].str.len() // 200).clip(0, 2)\n",
        "    train_data['stratification_key'] = (\n",
        "        train_data['response_length_category'].astype(str) + '_' + \n",
        "        train_data['prompt_complexity'].astype(str)\n",
        "    )\n",
        "    \n",
        "    # Check if stratification is viable\n",
        "    group_counts = train_data['stratification_key'].value_counts()\n",
        "    if group_counts.min() < 2:\n",
        "        print(\"⚠️ Using simple stratification due to small groups\")\n",
        "        stratify_column = train_data['response_length_category']\n",
        "    else:\n",
        "        print(\"✅ Using combined stratification\")\n",
        "        stratify_column = train_data['stratification_key']\n",
        "    \n",
        "    # Initialize cross-validation\n",
        "    cv_splitter = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "    \n",
        "    fold_models = []\n",
        "    fold_scores = []\n",
        "    \n",
        "    for fold_idx, (train_indices, val_indices) in enumerate(cv_splitter.split(train_data, stratify_column)):\n",
        "        print(f\"\\n🔄 Training Fold {fold_idx + 1}/{n_folds}\")\n",
        "        \n",
        "        # Split data\n",
        "        fold_train_data = train_data.iloc[train_indices]\n",
        "        fold_val_data = train_data.iloc[val_indices]\n",
        "        \n",
        "        print(f\"Fold {fold_idx + 1}: Training on {len(fold_train_data)} samples, validating on {len(fold_val_data)} samples\")\n",
        "        \n",
        "        # Load fresh model for this fold\n",
        "        fold_model, fold_tokenizer = load_clinical_model(model_name)\n",
        "        \n",
        "        # Create data collator for this fold\n",
        "        fold_data_collator = DataCollatorForSeq2Seq(\n",
        "            tokenizer=fold_tokenizer,\n",
        "            model=fold_model,\n",
        "            padding=True,\n",
        "            max_length=512,\n",
        "            label_pad_token_id=-100,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        \n",
        "        # Prepare datasets for this fold\n",
        "        fold_train_dataset = prepare_clinical_dataset(fold_train_data, fold_tokenizer, is_training=True)\n",
        "        fold_val_dataset = prepare_clinical_dataset(fold_val_data, fold_tokenizer, is_training=False)\n",
        "        \n",
        "        # Training arguments optimized for clinical text\n",
        "        training_config = Seq2SeqTrainingArguments(\n",
        "            output_dir=f\"./clinical_model_fold_{fold_idx}\",\n",
        "            overwrite_output_dir=True,\n",
        "            evaluation_strategy=\"steps\",\n",
        "            eval_steps=50,\n",
        "            save_strategy=\"steps\", \n",
        "            save_steps=100,\n",
        "            learning_rate=5e-5,\n",
        "            weight_decay=0.01,\n",
        "            per_device_train_batch_size=2,\n",
        "            per_device_eval_batch_size=4,\n",
        "            gradient_accumulation_steps=16,\n",
        "            num_train_epochs=4,  # Reduced for faster training\n",
        "            predict_with_generate=True,\n",
        "            generation_max_length=150,\n",
        "            warmup_steps=100,\n",
        "            fp16=torch.cuda.is_available(),\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_clinical_rouge\",\n",
        "            greater_is_better=True,\n",
        "            save_total_limit=2,\n",
        "            report_to=\"none\",\n",
        "            label_smoothing_factor=0.1,\n",
        "            # ❌ REMOVED length_penalty - invalid here\n",
        "        )\n",
        "        \n",
        "        # ✅ CORRECTED: Initialize trainer with correct variables\n",
        "        fold_trainer = Seq2SeqTrainer(\n",
        "            model=fold_model,                    # ✅ Correct fold model\n",
        "            args=training_config,                # ✅ Correct fold config  \n",
        "            train_dataset=fold_train_dataset,    # ✅ Correct fold train data\n",
        "            eval_dataset=fold_val_dataset,       # ✅ Correct fold val data\n",
        "            data_collator=fold_data_collator,    # ✅ Correct fold collator\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "        \n",
        "        print(f\"✅ Fold {fold_idx + 1} trainer initialized\")\n",
        "        \n",
        "        # Train the fold\n",
        "        fold_trainer.train()\n",
        "        \n",
        "        # Evaluate fold performance\n",
        "        fold_evaluation = fold_trainer.evaluate()\n",
        "        fold_score = fold_evaluation['eval_clinical_rouge']\n",
        "        \n",
        "        fold_models.append({\n",
        "            'fold': fold_idx,\n",
        "            'model_path': f\"./clinical_model_fold_{fold_idx}\",\n",
        "            'score': fold_score,\n",
        "            'model': fold_model,\n",
        "            'tokenizer': fold_tokenizer\n",
        "        })\n",
        "        fold_scores.append(fold_score)\n",
        "        \n",
        "        print(f\"Fold {fold_idx + 1} Score: {fold_score:.4f}\")\n",
        "        \n",
        "        # Save the model\n",
        "        fold_trainer.save_model(f\"./clinical_model_fold_{fold_idx}\")\n",
        "        fold_tokenizer.save_pretrained(f\"./clinical_model_fold_{fold_idx}\")\n",
        "        \n",
        "        # Clean up memory\n",
        "        del fold_trainer, fold_model, fold_tokenizer\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "    \n",
        "    # Print cross-validation summary\n",
        "    avg_score = np.mean(fold_scores)\n",
        "    std_score = np.std(fold_scores)\n",
        "    print(f\"\\n📊 Cross-Validation Results:\")\n",
        "    print(f\"Average Score: {avg_score:.4f} ± {std_score:.4f}\")\n",
        "    print(f\"Best Fold Score: {max(fold_scores):.4f}\")\n",
        "    print(f\"Worst Fold Score: {min(fold_scores):.4f}\")\n",
        "    \n",
        "    # Sort models by performance\n",
        "    fold_models.sort(key=lambda x: x['score'], reverse=True)\n",
        "    \n",
        "    return fold_models\n",
        "\n",
        "# Rest of the helper functions remain the same...\n",
        "def load_clinical_model(model_name):\n",
        "    \"\"\"Load and configure model for clinical reasoning tasks\"\"\"\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "    \n",
        "    model = model.to(device)\n",
        "    model.gradient_checkpointing_enable()\n",
        "    model.config.use_cache = False\n",
        "    \n",
        "    return model, tokenizer\n",
        "\n",
        "def prepare_clinical_dataset(dataframe, tokenizer, is_training=True):\n",
        "    \"\"\"Prepare dataset specifically for clinical reasoning training\"\"\"\n",
        "    \n",
        "    def tokenize_clinical_examples(examples):\n",
        "        model_inputs = tokenizer(\n",
        "            [\"clinical reasoning: \" + text for text in examples['Clinical_Reasoning_Prompt']],\n",
        "            max_length=512,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=None\n",
        "        )\n",
        "        \n",
        "        if is_training:\n",
        "            with tokenizer.as_target_tokenizer():\n",
        "                labels = tokenizer(\n",
        "                    examples['Clinician'],\n",
        "                    max_length=150,\n",
        "                    padding='max_length',\n",
        "                    truncation=True,\n",
        "                    return_tensors=None\n",
        "                )\n",
        "            \n",
        "            labels[\"input_ids\"] = [\n",
        "                [(l if l != tokenizer.pad_token_id else -100) for l in label] \n",
        "                for label in labels[\"input_ids\"]\n",
        "            ]\n",
        "            \n",
        "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "        \n",
        "        return model_inputs\n",
        "    \n",
        "    dataset_columns = ['Clinical_Reasoning_Prompt', 'Clinician'] if is_training else ['Clinical_Reasoning_Prompt']\n",
        "    clinical_dataset = Dataset.from_pandas(dataframe[dataset_columns])\n",
        "    \n",
        "    tokenized_dataset = clinical_dataset.map(\n",
        "        tokenize_clinical_examples,\n",
        "        batched=True,\n",
        "        batch_size=16,\n",
        "        remove_columns=dataset_columns,\n",
        "        desc=f\"Tokenizing {'training' if is_training else 'validation'} data\"\n",
        "    )\n",
        "    \n",
        "    return tokenized_dataset\n",
        "\n",
        "# Train multiple models with cross-validation\n",
        "print(\"🏥 Starting Clinical Reasoning Model Training with Cross-Validation\")\n",
        "trained_models = train_clinical_reasoning_model_with_cross_validation(\n",
        "    train_df, \n",
        "    model_name=\"google/flan-t5-large\",\n",
        "    n_folds=3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bed7fbeb",
      "metadata": {
        "id": "bed7fbeb"
      },
      "source": [
        "## Generate Test Predictions\n",
        "\n",
        "Generate predictions for the test set and prepare the submission file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c15ab717",
      "metadata": {
        "id": "c15ab717"
      },
      "outputs": [],
      "source": [
        "## Clear gpu memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6f82c6c",
      "metadata": {
        "id": "f6f82c6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating predictions for test set...\n",
            "Loading and optimizing model...\n",
            "Using torch.compile for model acceleration\n",
            "Pre-processing prompts...\n",
            "Using dynamic batch size: 12 (based on 2.87GB free GPU memory)\n",
            "Starting batch inference...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "025b9d611f054e478a2a3ffdc2058d61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating predictions:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\mirac\\AppData\\Local\\Temp\\ipykernel_25800\\2487824345.py:103: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.inference_mode(), torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Inference completed in 36.14 seconds\n",
            "Average time per sample: 361.36 ms\n",
            "Samples per second: 2.77\n",
            "Submission file saved to submission.csv\n",
            "\n",
            "Sample predictions:\n",
            "Sample 1:\n",
            "Prediction: summary a 24 year old female complains of sharp pain in the right side of the nose that started 2 days ago and has been gradually worsening no past medical history on assessment there is tenderness on palpation on the left side the nasal bridge no visible signs of inflammation or infection vitals bp 129 81 mmhg pr 80 rr 20 t 36 8 spo2 94 what could be the diagnosis of this patient if the patient is unable to perform surgery he should be referred to gynecology\n",
            "--------------------------------------------------\n",
            "Sample 2:\n",
            "Prediction: summary a 3 year old boy brought to the facility had inserted bean seed on the right nostrils on exam it was deep how do i remove it do I need to refer the patient to an emergency department for further evaluation if the seed is removed the child will be discharged from the hospital for resuscitation and discharge he will have to undergo an intravenous syringe to remove the nasopharyngeal x rays to check for infection and to monitor for signs of infection\n",
            "--------------------------------------------------\n",
            "Sample 3:\n",
            "Prediction: summary a 22 year old man brought in with limb weakness which increased in severity over two weeks followed by loss of sensation and difficulty in breathing on assessment he was unable to move vitals t 38 c pr 102 rr 21 breaths min spo 72 diagnosis 2 how will the mother handle the patient if it s serious disease\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Generate predictions on test set\n",
        "def generate_test_predictions():\n",
        "    \"\"\"Generate predictions for the test dataset with ultra-optimized inference\"\"\"\n",
        "    print(\"Generating predictions for test set...\")\n",
        "\n",
        "    # Set up for optimal performance\n",
        "    if torch.cuda.is_available():\n",
        "        # Set CUDA optimization flags\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        torch.backends.cudnn.deterministic = False\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True  # Allow TF32 on Ampere GPUs\n",
        "        if hasattr(torch.backends.cudnn, \"allow_tf32\"):\n",
        "            torch.backends.cudnn.allow_tf32 = True    # Faster CUDNN ops with TF32\n",
        "\n",
        "    # Memory optimization for inference\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    # Load the best model\n",
        "    print(\"Loading and optimizing model...\")\n",
        "    best_model = T5ForConditionalGeneration.from_pretrained(\n",
        "        \"./clinical_reasoning_final_model\",\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "    ).to(device)\n",
        "\n",
        "    # Enable optimizations\n",
        "    best_model.eval()\n",
        "    best_model.config.use_cache = True  # Enable KV cache for generation\n",
        "\n",
        "    # Enable inference mode for maximum optimization\n",
        "    torch._C._jit_set_profiling_executor(False)\n",
        "    torch._C._jit_set_profiling_mode(False)\n",
        "\n",
        "    # Use torch.compile if available (PyTorch 2.0+)\n",
        "    try:\n",
        "        if hasattr(torch, 'compile') and torch.cuda.is_available():\n",
        "            print(\"Using torch.compile for model acceleration\")\n",
        "            best_model = torch.compile(best_model, mode=\"reduce-overhead\")\n",
        "    except Exception as e:\n",
        "        print(f\"Torch compile not available: {e}\")\n",
        "\n",
        "    # Optimize tokenizer settings\n",
        "    tokenizer_kwargs = {\n",
        "        \"padding\": True,\n",
        "        \"truncation\": True,\n",
        "        \"max_length\": MAX_SOURCE_LENGTH,\n",
        "        \"return_tensors\": \"pt\"\n",
        "    }\n",
        "\n",
        "    # Prepare prompts in advance to optimize memory access\n",
        "    print(\"Pre-processing prompts...\")\n",
        "    prefix = \"clinical reasoning: \"\n",
        "    prompts = [prefix + text for text in test_df['Clinical_Reasoning_Prompt']]\n",
        "\n",
        "    # Tokenize all prompts in one batch for efficiency\n",
        "    test_inputs = tokenizer(prompts, **tokenizer_kwargs)\n",
        "\n",
        "    # Pre-allocate result list\n",
        "    test_predictions = []\n",
        "\n",
        "    # Determine optimal batch size based on available memory\n",
        "    if torch.cuda.is_available():\n",
        "        free_mem = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()\n",
        "        free_mem_gb = free_mem / (1024 ** 3)\n",
        "        # Scale batch size with available memory\n",
        "        batch_size = min(int(free_mem_gb * 4) + 1, 32)  # Up to 32 with headroom\n",
        "        batch_size = max(batch_size, 8)  # At least 8\n",
        "        print(f\"Using dynamic batch size: {batch_size} (based on {free_mem_gb:.2f}GB free GPU memory)\")\n",
        "    else:\n",
        "        batch_size = 4\n",
        "        print(f\"Using CPU batch size: {batch_size}\")\n",
        "\n",
        "    # Configure generation parameters based on speed vs quality trade-off\n",
        "    generation_config = {\n",
        "        \"max_length\": MAX_TARGET_LENGTH,\n",
        "        \"min_length\": 10,\n",
        "        \"num_beams\": 2,        # Reduced from 4 to 2 for speed\n",
        "        \"early_stopping\": True,\n",
        "        \"no_repeat_ngram_size\": 2,\n",
        "        \"length_penalty\": 1.0,\n",
        "        \"use_cache\": True      # Enable KV cache\n",
        "    }\n",
        "\n",
        "    # Measure prediction time\n",
        "    print(\"Starting batch inference...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Using tqdm for progress tracking\n",
        "    with tqdm(total=len(test_df), desc=\"Generating predictions\") as pbar:\n",
        "        for i in range(0, len(test_df), batch_size):\n",
        "            # Get batch\n",
        "            current_batch_size = min(batch_size, len(test_df) - i)\n",
        "            batch_inputs = {\n",
        "                'input_ids': test_inputs['input_ids'][i:i+current_batch_size].to(device),\n",
        "                'attention_mask': test_inputs['attention_mask'][i:i+current_batch_size].to(device),\n",
        "            }\n",
        "\n",
        "            # Set up auto-cast context for mixed precision\n",
        "            with torch.inference_mode(), torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "                try:\n",
        "                    # Generate with optimized parameters\n",
        "                    outputs = best_model.generate(\n",
        "                        **batch_inputs,\n",
        "                        **generation_config\n",
        "                    )\n",
        "\n",
        "                    # Decode and process batch outputs\n",
        "                    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "                    test_predictions.extend(decoded_outputs)\n",
        "\n",
        "                except RuntimeError as e:\n",
        "                    if 'out of memory' in str(e).lower():\n",
        "                        # Handle OOM by reducing batch size and retrying\n",
        "                        print(f\"\\nOOM error at batch {i}, reducing batch and retrying...\")\n",
        "                        torch.cuda.empty_cache()\n",
        "                        current_batch_size = max(1, current_batch_size // 2)\n",
        "\n",
        "                        for j in range(i, min(i + batch_size, len(test_df)), current_batch_size):\n",
        "                            sub_batch = {\n",
        "                                'input_ids': test_inputs['input_ids'][j:j+current_batch_size].to(device),\n",
        "                                'attention_mask': test_inputs['attention_mask'][j:j+current_batch_size].to(device),\n",
        "                            }\n",
        "\n",
        "                            # Use even more aggressive parameters for recovery\n",
        "                            reduced_config = generation_config.copy()\n",
        "                            reduced_config[\"num_beams\"] = 1  # Use greedy search\n",
        "\n",
        "                            with torch.inference_mode(), torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "                                outputs = best_model.generate(**sub_batch, **reduced_config)\n",
        "                                decoded_sub_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "                                test_predictions.extend(decoded_sub_outputs)\n",
        "                                pbar.update(len(sub_batch['input_ids']))\n",
        "                    else:\n",
        "                        raise e\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.update(current_batch_size)\n",
        "\n",
        "            # Optional: release memory after each batch\n",
        "            if i % (batch_size * 4) == 0 and torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    # Calculate inference metrics\n",
        "    total_time = time.time() - start_time\n",
        "    avg_time_per_sample = total_time / len(test_df) * 1000  # in milliseconds\n",
        "\n",
        "    print(f\"\\nInference completed in {total_time:.2f} seconds\")\n",
        "    print(f\"Average time per sample: {avg_time_per_sample:.2f} ms\")\n",
        "    print(f\"Samples per second: {len(test_df)/total_time:.2f}\")\n",
        "\n",
        "    # Create submission DataFrame using 'Master_Index' as the ID column\n",
        "    submission_df = pd.DataFrame({\n",
        "        'ID': test_df['Master_Index'],\n",
        "        'Clinician': test_predictions\n",
        "    })\n",
        "\n",
        "    # Save submission file\n",
        "    submission_path = \"submission.csv\"\n",
        "    submission_df.to_csv(submission_path, index=False)\n",
        "    print(f\"Submission file saved to {submission_path}\")\n",
        "\n",
        "    # Show sample predictions\n",
        "    print(\"\\nSample predictions:\")\n",
        "    for i in range(min(3, len(test_predictions))):\n",
        "        print(f\"Sample {i+1}:\")\n",
        "        print(f\"Prediction: {test_predictions[i]}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    # Clean up memory\n",
        "    del best_model\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    return submission_df\n",
        "\n",
        "# Generate test predictions\n",
        "submission = generate_test_predictions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "917f5a29",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add this for ensemble predictions\n",
        "\n",
        "class ClinicalReasoningEnsemble:\n",
        "    \"\"\"Ensemble system for clinical reasoning predictions\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.models = []\n",
        "        self.model_weights = []\n",
        "    \n",
        "    def add_trained_model(self, model_path, weight=1.0):\n",
        "        \"\"\"Add a trained model to the ensemble\"\"\"\n",
        "        try:\n",
        "            model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "            tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "            \n",
        "            model = model.to(device)\n",
        "            model.eval()\n",
        "            \n",
        "            self.models.append({\n",
        "                'model': model,\n",
        "                'tokenizer': tokenizer,\n",
        "                'path': model_path,\n",
        "                'weight': weight\n",
        "            })\n",
        "            self.model_weights.append(weight)\n",
        "            \n",
        "            print(f\"✅ Added model from {model_path} with weight {weight}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to load model from {model_path}: {e}\")\n",
        "    \n",
        "    def generate_ensemble_predictions(self, test_prompts, max_length=150):\n",
        "        \"\"\"Generate predictions using ensemble of models\"\"\"\n",
        "        print(f\"🔄 Generating predictions with {len(self.models)} models in ensemble...\")\n",
        "        \n",
        "        all_model_predictions = []\n",
        "        \n",
        "        # Generate predictions from each model\n",
        "        for i, model_info in enumerate(self.models):\n",
        "            print(f\"Generating predictions with model {i+1}/{len(self.models)}...\")\n",
        "            \n",
        "            model = model_info['model']\n",
        "            tokenizer = model_info['tokenizer']\n",
        "            \n",
        "            model_predictions = self._generate_single_model_predictions(\n",
        "                model, tokenizer, test_prompts, max_length\n",
        "            )\n",
        "            \n",
        "            all_model_predictions.append(model_predictions)\n",
        "        \n",
        "        # Combine predictions intelligently\n",
        "        final_predictions = self._combine_predictions_intelligently(all_model_predictions)\n",
        "        \n",
        "        return final_predictions\n",
        "    \n",
        "    def _generate_single_model_predictions(self, model, tokenizer, prompts, max_length):\n",
        "        \"\"\"Generate predictions from a single model\"\"\"\n",
        "        predictions = []\n",
        "        batch_size = 8\n",
        "        \n",
        "        # Add clinical reasoning prefix\n",
        "        formatted_prompts = [\"clinical reasoning: \" + prompt for prompt in prompts]\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(formatted_prompts), batch_size):\n",
        "                batch_prompts = formatted_prompts[i:i+batch_size]\n",
        "                \n",
        "                # Tokenize batch\n",
        "                inputs = tokenizer(\n",
        "                    batch_prompts,\n",
        "                    max_length=512,\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    return_tensors=\"pt\"\n",
        "                ).to(device)\n",
        "                \n",
        "                # Generate with multiple strategies for diversity\n",
        "                generation_configs = [\n",
        "                    {'num_beams': 4, 'temperature': 0.7, 'do_sample': True, 'top_p': 0.9},\n",
        "                    {'num_beams': 3, 'temperature': 0.8, 'do_sample': True, 'top_k': 50},\n",
        "                    {'num_beams': 5, 'length_penalty': 1.2, 'do_sample': False}\n",
        "                ]\n",
        "                \n",
        "                # Use first config as default, can be made more sophisticated\n",
        "                config = generation_configs[0]\n",
        "                \n",
        "                # Generate\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_length=max_length,\n",
        "                    **config\n",
        "                )\n",
        "                \n",
        "                # Decode\n",
        "                batch_predictions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "                predictions.extend(batch_predictions)\n",
        "        \n",
        "        return predictions\n",
        "    \n",
        "    def _combine_predictions_intelligently(self, all_predictions):\n",
        "        \"\"\"Intelligently combine predictions from multiple models\"\"\"\n",
        "        final_predictions = []\n",
        "        \n",
        "        for i in range(len(all_predictions[0])):\n",
        "            # Get all predictions for this sample\n",
        "            sample_predictions = [model_preds[i] for model_preds in all_predictions]\n",
        "            \n",
        "            # Score each prediction\n",
        "            prediction_scores = []\n",
        "            for pred in sample_predictions:\n",
        "                score = self._calculate_prediction_quality_score(pred)\n",
        "                prediction_scores.append(score)\n",
        "            \n",
        "            # Select best prediction (weighted by model performance if available)\n",
        "            if len(self.model_weights) == len(prediction_scores):\n",
        "                weighted_scores = [score * weight for score, weight in zip(prediction_scores, self.model_weights)]\n",
        "                best_idx = np.argmax(weighted_scores)\n",
        "            else:\n",
        "                best_idx = np.argmax(prediction_scores)\n",
        "            \n",
        "            final_predictions.append(sample_predictions[best_idx])\n",
        "        \n",
        "        return final_predictions\n",
        "    \n",
        "    def _calculate_prediction_quality_score(self, prediction):\n",
        "        \"\"\"Calculate quality score for prediction selection\"\"\"\n",
        "        score = 0.0\n",
        "        \n",
        "        # Length score (prefer responses in optimal range)\n",
        "        word_count = len(prediction.split())\n",
        "        if 20 <= word_count <= 80:\n",
        "            score += 2.0\n",
        "        elif 15 <= word_count <= 100:\n",
        "            score += 1.0\n",
        "        \n",
        "        # Clinical terminology score\n",
        "        clinical_terms = [\n",
        "            'assess', 'evaluate', 'patient', 'clinical', 'recommend', \n",
        "            'consider', 'treatment', 'diagnosis', 'monitor', 'care'\n",
        "        ]\n",
        "        clinical_term_count = sum(1 for term in clinical_terms if term in prediction.lower())\n",
        "        score += clinical_term_count * 0.5\n",
        "        \n",
        "        # Structure and organization score\n",
        "        if any(marker in prediction for marker in ['1.', '2.', '•', '-', ':']):\n",
        "            score += 1.0\n",
        "        \n",
        "        # Coherence score (avoid repetition)\n",
        "        words = prediction.lower().split()\n",
        "        if words:\n",
        "            unique_ratio = len(set(words)) / len(words)\n",
        "            score += unique_ratio * 1.0\n",
        "        \n",
        "        # Clinical reasoning indicators\n",
        "        reasoning_words = ['because', 'due to', 'therefore', 'indicates', 'suggests', 'based on']\n",
        "        reasoning_count = sum(1 for word in reasoning_words if word in prediction.lower())\n",
        "        score += reasoning_count * 0.3\n",
        "        \n",
        "        return score\n",
        "\n",
        "def create_winning_ensemble_submission():\n",
        "    \"\"\"Create ensemble submission using best trained models\"\"\"\n",
        "    print(\"🏆 Creating WINNING ensemble submission...\")\n",
        "    \n",
        "    # Initialize ensemble\n",
        "    clinical_ensemble = ClinicalReasoningEnsemble()\n",
        "    \n",
        "    # Add the best models from cross-validation\n",
        "    # Sort trained models by score and add top performers\n",
        "    best_models = sorted(trained_models, key=lambda x: x['score'], reverse=True)[:3]\n",
        "    \n",
        "    for model_info in best_models:\n",
        "        clinical_ensemble.add_trained_model(\n",
        "            model_info['model_path'], \n",
        "            weight=model_info['score']  # Weight by performance\n",
        "        )\n",
        "    \n",
        "    # Generate ensemble predictions\n",
        "    ensemble_predictions = clinical_ensemble.generate_ensemble_predictions(\n",
        "        test_df['Clinical_Reasoning_Prompt'].tolist(),\n",
        "        max_length=150\n",
        "    )\n",
        "    \n",
        "    # Post-process predictions for clinical quality\n",
        "    processed_predictions = [\n",
        "        post_process_clinical_prediction(pred) \n",
        "        for pred in ensemble_predictions\n",
        "    ]\n",
        "    \n",
        "    # Create submission\n",
        "    submission_df = pd.DataFrame({\n",
        "        'ID': test_df['Master_Index'],\n",
        "        'Clinician': processed_predictions\n",
        "    })\n",
        "    \n",
        "    # Save submission with timestamp\n",
        "    timestamp = int(time.time())\n",
        "    submission_path = f\"clinical_ensemble_submission_{timestamp}.csv\"\n",
        "    submission_df.to_csv(submission_path, index=False)\n",
        "    \n",
        "    print(f\"🎯 Ensemble submission saved: {submission_path}\")\n",
        "    \n",
        "    # Display sample predictions\n",
        "    print(\"\\n📋 Sample ensemble predictions:\")\n",
        "    for i in range(min(3, len(processed_predictions))):\n",
        "        print(f\"\\nSample {i+1}:\")\n",
        "        print(f\"Prediction: {processed_predictions[i][:200]}...\")\n",
        "        print(\"-\" * 80)\n",
        "    \n",
        "    return submission_df\n",
        "\n",
        "def post_process_clinical_prediction(prediction):\n",
        "    \"\"\"Post-process prediction for clinical quality and formatting\"\"\"\n",
        "    # Clean up text\n",
        "    cleaned = re.sub(r'\\s+', ' ', prediction).strip()\n",
        "    \n",
        "    # Ensure proper sentence ending\n",
        "    if cleaned and not cleaned.endswith('.'):\n",
        "        cleaned += '.'\n",
        "    \n",
        "    # Capitalize first letter\n",
        "    if cleaned:\n",
        "        cleaned = cleaned[0].upper() + cleaned[1:]\n",
        "    \n",
        "    # Ensure minimum quality\n",
        "    if len(cleaned.split()) < 8:\n",
        "        cleaned += \" Additional clinical assessment and monitoring should be considered based on patient presentation and clinical guidelines.\"\n",
        "    \n",
        "    return cleaned\n",
        "\n",
        "# Execute the ensemble prediction\n",
        "winning_submission = create_winning_ensemble_submission()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a489e139",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_multiple_submissions():\n",
        "    \"\"\"Create multiple submission variants for competition strategy\"\"\"\n",
        "    print(\"🎯 Creating multiple submission strategies...\")\n",
        "    \n",
        "    submissions = {}\n",
        "    \n",
        "    # Submission 1: Single best model (from main training)\n",
        "    print(\"\\n1️⃣ Creating single model submission...\")\n",
        "    try:\n",
        "        single_model_preds = generate_test_predictions()\n",
        "        submissions['single_best'] = single_model_preds\n",
        "        print(\"✅ Single model submission created\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Single model failed: {e}\")\n",
        "    \n",
        "    # Submission 2: Ensemble (if cross-validation models available)\n",
        "    print(\"\\n2️⃣ Creating ensemble submission...\")\n",
        "    try:\n",
        "        if 'trained_models' in globals() and trained_models:\n",
        "            ensemble_preds = create_winning_ensemble_submission()\n",
        "            submissions['ensemble'] = ensemble_preds\n",
        "            print(\"✅ Ensemble submission created\")\n",
        "        else:\n",
        "            print(\"⚠️ No trained models available for ensemble\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ensemble failed: {e}\")\n",
        "    \n",
        "    # Submission 3: Conservative (shorter, safer responses)\n",
        "    print(\"\\n3️⃣ Creating conservative submission...\")\n",
        "    try:\n",
        "        conservative_preds = generate_conservative_predictions()\n",
        "        submissions['conservative'] = conservative_preds\n",
        "        print(\"✅ Conservative submission created\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Conservative failed: {e}\")\n",
        "    \n",
        "    print(f\"\\n🎯 Created {len(submissions)} submission variants\")\n",
        "    return submissions\n",
        "\n",
        "def generate_conservative_predictions():\n",
        "    \"\"\"Generate shorter, more conservative clinical responses\"\"\"\n",
        "    print(\"Generating conservative predictions...\")\n",
        "    \n",
        "    # Load the best model\n",
        "    best_model = T5ForConditionalGeneration.from_pretrained(\n",
        "        \"./clinical_reasoning_final_model\",\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    ).to(device)\n",
        "    best_model.eval()\n",
        "    \n",
        "    # Conservative generation parameters\n",
        "    conservative_config = {\n",
        "        \"max_length\": 100,  # Shorter responses\n",
        "        \"min_length\": 15,\n",
        "        \"num_beams\": 3,\n",
        "        \"length_penalty\": 1.5,  # Strongly prefer shorter\n",
        "        \"repetition_penalty\": 1.2,  # Avoid repetition\n",
        "        \"early_stopping\": True,\n",
        "    }\n",
        "    \n",
        "    # Generate predictions\n",
        "    test_predictions = []\n",
        "    batch_size = 8\n",
        "    \n",
        "    prefix = \"clinical reasoning: \"\n",
        "    prompts = [prefix + text for text in test_df['Clinical_Reasoning_Prompt']]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(prompts), batch_size):\n",
        "            batch_prompts = prompts[i:i+batch_size]\n",
        "            \n",
        "            inputs = tokenizer(\n",
        "                batch_prompts,\n",
        "                max_length=512,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(device)\n",
        "            \n",
        "            outputs = best_model.generate(**inputs, **conservative_config)\n",
        "            batch_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "            test_predictions.extend(batch_preds)\n",
        "    \n",
        "    # Create conservative submission\n",
        "    submission_df = pd.DataFrame({\n",
        "        'ID': test_df['Master_Index'],\n",
        "        'Clinician': test_predictions\n",
        "    })\n",
        "    \n",
        "    timestamp = int(time.time())\n",
        "    submission_path = f\"conservative_submission_{timestamp}.csv\"\n",
        "    submission_df.to_csv(submission_path, index=False)\n",
        "    print(f\"Conservative submission saved: {submission_path}\")\n",
        "    \n",
        "    del best_model\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    return submission_df\n",
        "\n",
        "# Execute multiple submission strategy\n",
        "if __name__ == \"__main__\":\n",
        "    multiple_submissions = create_multiple_submissions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f836ca04",
      "metadata": {
        "id": "f836ca04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating ultra-fast predictions for test set...\n",
            "Loading and hyper-optimizing model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mirac\\anaconda3\\Lib\\site-packages\\torch\\cuda\\__init__.py:1150: UserWarning: Synchronization debug mode is a prototype feature and does not yet detect all synchronizing operations (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\cuda\\Module.cpp:980.)\n",
            "  torch._C._cuda_set_sync_debug_mode(debug_mode)\n",
            "c:\\Users\\mirac\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\mirac\\anaconda3\\Lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying torch.compile with maximum optimization\n",
            "Pre-processing with consistent tokenization...\n",
            "Tokenizing all prompts with consistent padding...\n",
            "Tokenized shape: torch.Size([100, 334])\n",
            "Using ultra-aggressive batch size: 64\n",
            "Warming up model and CUDA kernels...\n",
            "Warmup completed!\n",
            "Starting ultra-fast batch inference...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ultra-fast inference:   0%|          | 0/100 [00:00<?, ?it/s]C:\\Users\\mirac\\AppData\\Local\\Temp\\ipykernel_25800\\3591250219.py:146: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available(), dtype=torch.float16):\n",
            "c:\\Users\\mirac\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:418: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\mirac\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:437: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `0.8` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
            "  warnings.warn(\n",
            "Ultra-fast inference: 100%|██████████| 100/100 [00:07<00:00, 13.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🚀 ULTRA-FAST INFERENCE COMPLETED!\n",
            "Total time: 7.42 seconds\n",
            "Average time per vignette: 74.2 ms\n",
            "Samples per second: 13.5\n",
            "Target achieved: ✅ YES\n",
            "Submission saved: submission_ultrafast_1750616767.csv\n",
            "\n",
            "Sample ultra-fast predictions:\n",
            "Sample 1: summary a 24 year old female complains of sharp pain in the right side of the nose that started 2 da...\n",
            "Sample 2: summary a 3 year old boy brought to the facility had inserted a bean seed on the right nostrils on e...\n",
            "Sample 3: summary 22 year old male with a history of weakness of the lower limbs which increased in severity o...\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import threading\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import queue\n",
        "\n",
        "def generate_test_predictions():\n",
        "    \"\"\"Generate predictions for the test dataset with ultra-optimized inference (<50ms per vignette)\"\"\"\n",
        "    print(\"Generating ultra-fast predictions for test set...\")\n",
        "\n",
        "    # Maximum performance CUDA settings\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        torch.backends.cudnn.deterministic = False\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "        torch.set_float32_matmul_precision('medium')  # Use TensorFloat-32\n",
        "\n",
        "        # Enable graph capture for maximum speed\n",
        "        torch.cuda.set_sync_debug_mode(0)\n",
        "\n",
        "    # Aggressive memory cleanup\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    # Load model with maximum optimization\n",
        "    print(\"Loading and hyper-optimizing model...\")\n",
        "    best_model = T5ForConditionalGeneration.from_pretrained(\n",
        "        \"./clinical_reasoning_final_model\",\n",
        "        torch_dtype=torch.float16,  # Force FP16 for speed\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "    ).to(device)\n",
        "\n",
        "    # Aggressive model optimizations\n",
        "    best_model.eval()\n",
        "    best_model.config.use_cache = True\n",
        "    best_model.config.output_attentions = False\n",
        "    best_model.config.output_hidden_states = False\n",
        "    best_model.config.return_dict = False\n",
        "\n",
        "    # Disable gradient computation globally\n",
        "    torch.set_grad_enabled(False)\n",
        "\n",
        "    # Use fastest tokenizer\n",
        "    fast_tokenizer = tokenizer\n",
        "\n",
        "    # Torch compile with maximum optimization\n",
        "    try:\n",
        "        if hasattr(torch, 'compile') and torch.cuda.is_available():\n",
        "            print(\"Applying torch.compile with maximum optimization\")\n",
        "            best_model = torch.compile(\n",
        "                best_model,\n",
        "                mode=\"max-autotune\",  # Maximum speed optimization\n",
        "                fullgraph=True,\n",
        "                dynamic=False\n",
        "            )\n",
        "    except Exception as e:\n",
        "        print(f\"Advanced torch compile not available: {e}\")\n",
        "        # Fallback to basic compile\n",
        "        try:\n",
        "            best_model = torch.compile(best_model, mode=\"reduce-overhead\")\n",
        "        except:\n",
        "            print(\"Using model without torch.compile\")\n",
        "\n",
        "    # Pre-process ALL data at once with consistent padding\n",
        "    print(\"Pre-processing with consistent tokenization...\")\n",
        "    prefix = \"clinical reasoning: \"\n",
        "    prompts = [prefix + text for text in test_df['Clinical_Reasoning_Prompt']]\n",
        "\n",
        "    # Tokenize ALL prompts at once to ensure consistent padding\n",
        "    print(\"Tokenizing all prompts with consistent padding...\")\n",
        "    all_tokenized = fast_tokenizer(\n",
        "        prompts,\n",
        "        padding=True,  # This ensures all sequences have the same length\n",
        "        truncation=True,\n",
        "        max_length=MAX_SOURCE_LENGTH,\n",
        "        return_tensors=\"pt\",\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "    all_input_ids = all_tokenized['input_ids']\n",
        "    all_attention_masks = all_tokenized['attention_mask']\n",
        "\n",
        "    print(f\"Tokenized shape: {all_input_ids.shape}\")\n",
        "\n",
        "    # Ultra-aggressive batch size calculation\n",
        "    if torch.cuda.is_available():\n",
        "        # Calculate max batch size based on GPU memory\n",
        "        total_mem = torch.cuda.get_device_properties(0).total_memory\n",
        "        free_mem = total_mem - torch.cuda.memory_allocated()\n",
        "\n",
        "        # Estimate memory per sample (very aggressive)\n",
        "        sample_mem_estimate = MAX_SOURCE_LENGTH * 2 * 4  # rough estimate in bytes\n",
        "        max_theoretical_batch = int(free_mem * 0.8 / sample_mem_estimate)\n",
        "        batch_size = min(max_theoretical_batch, 64)  # Cap at 64\n",
        "        batch_size = max(batch_size, 16)  # Minimum 16 for efficiency\n",
        "\n",
        "        print(f\"Using ultra-aggressive batch size: {batch_size}\")\n",
        "    else:\n",
        "        batch_size = 8\n",
        "\n",
        "    # Ultra-fast generation config (sacrifice quality for speed)\n",
        "    fast_generation_config = {\n",
        "        \"max_length\": min(MAX_TARGET_LENGTH, 128),  # Reduce max length\n",
        "        \"min_length\": 5,\n",
        "        \"num_beams\": 1,           # Greedy search (fastest)\n",
        "        \"do_sample\": False,       # No sampling\n",
        "        \"early_stopping\": True,\n",
        "        \"use_cache\": True,\n",
        "        \"pad_token_id\": fast_tokenizer.pad_token_id,\n",
        "        \"eos_token_id\": fast_tokenizer.eos_token_id,\n",
        "        \"no_repeat_ngram_size\": 0,  # Disable for speed\n",
        "        \"repetition_penalty\": 1.0,\n",
        "        \"length_penalty\": 0.8     # Prefer shorter outputs\n",
        "    }\n",
        "\n",
        "    # Warmup the model for consistent timing\n",
        "    warmup_model(best_model, fast_tokenizer, device)\n",
        "\n",
        "    # Pre-allocate output tensors for maximum speed\n",
        "    test_predictions = []\n",
        "\n",
        "    print(\"Starting ultra-fast batch inference...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    with tqdm(total=len(test_df), desc=\"Ultra-fast inference\") as pbar:\n",
        "        for i in range(0, len(test_df), batch_size):\n",
        "            current_batch_size = min(batch_size, len(test_df) - i)\n",
        "\n",
        "            # Get batch tensors\n",
        "            batch_input_ids = all_input_ids[i:i+current_batch_size].to(device, non_blocking=True)\n",
        "            batch_attention_mask = all_attention_masks[i:i+current_batch_size].to(device, non_blocking=True)\n",
        "\n",
        "            # Use inference mode with autocast for maximum speed\n",
        "            with torch.inference_mode():\n",
        "                with torch.cuda.amp.autocast(enabled=torch.cuda.is_available(), dtype=torch.float16):\n",
        "                    try:\n",
        "                        # Direct model call for maximum speed\n",
        "                        outputs = best_model.generate(\n",
        "                            input_ids=batch_input_ids,\n",
        "                            attention_mask=batch_attention_mask,\n",
        "                            **fast_generation_config\n",
        "                        )\n",
        "\n",
        "                        # Fast decoding\n",
        "                        batch_predictions = fast_tokenizer.batch_decode(\n",
        "                            outputs,\n",
        "                            skip_special_tokens=True,\n",
        "                            clean_up_tokenization_spaces=False  # Skip cleanup for speed\n",
        "                        )\n",
        "\n",
        "                        test_predictions.extend(batch_predictions)\n",
        "\n",
        "                    except RuntimeError as e:\n",
        "                        if 'out of memory' in str(e).lower():\n",
        "                            print(f\"\\nOOM at batch {i}, using emergency fallback...\")\n",
        "                            torch.cuda.empty_cache()\n",
        "\n",
        "                            # Emergency single-sample processing\n",
        "                            for j in range(current_batch_size):\n",
        "                                single_input = batch_input_ids[j:j+1]\n",
        "                                single_mask = batch_attention_mask[j:j+1]\n",
        "\n",
        "                                with torch.inference_mode():\n",
        "                                    single_output = best_model.generate(\n",
        "                                        input_ids=single_input,\n",
        "                                        attention_mask=single_mask,\n",
        "                                        max_length=64,  # Even shorter for emergency\n",
        "                                        num_beams=1,\n",
        "                                        do_sample=False\n",
        "                                    )\n",
        "\n",
        "                                    single_pred = fast_tokenizer.decode(\n",
        "                                        single_output[0],\n",
        "                                        skip_special_tokens=True\n",
        "                                    )\n",
        "                                    test_predictions.append(single_pred)\n",
        "                        else:\n",
        "                            raise e\n",
        "\n",
        "            pbar.update(current_batch_size)\n",
        "\n",
        "            # Minimal memory management\n",
        "            if i % (batch_size * 8) == 0 and torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    # Performance metrics\n",
        "    total_time = time.time() - start_time\n",
        "    avg_time_per_sample = (total_time / len(test_df)) * 1000  # milliseconds\n",
        "    samples_per_second = len(test_df) / total_time\n",
        "\n",
        "    print(f\"\\n🚀 ULTRA-FAST INFERENCE COMPLETED!\")\n",
        "    print(f\"Total time: {total_time:.2f} seconds\")\n",
        "    print(f\"Average time per vignette: {avg_time_per_sample:.1f} ms\")\n",
        "    print(f\"Samples per second: {samples_per_second:.1f}\")\n",
        "    print(f\"Target achieved: {'✅ YES' if avg_time_per_sample < 100 else '❌ NO'}\")\n",
        "\n",
        "    # Create submission\n",
        "    submission_df = pd.DataFrame({\n",
        "        'ID': test_df['Master_Index'],\n",
        "        'Clinician': test_predictions\n",
        "    })\n",
        "\n",
        "    # Save with timestamp for tracking\n",
        "    timestamp = int(time.time())\n",
        "    submission_path = f\"submission_ultrafast_{timestamp}.csv\"\n",
        "    submission_df.to_csv(submission_path, index=False)\n",
        "    print(f\"Submission saved: {submission_path}\")\n",
        "\n",
        "    # Sample outputs\n",
        "    print(\"\\nSample ultra-fast predictions:\")\n",
        "    for i in range(min(3, len(test_predictions))):\n",
        "        print(f\"Sample {i+1}: {test_predictions[i][:100]}...\")\n",
        "\n",
        "    # Cleanup\n",
        "    del best_model, all_input_ids, all_attention_masks\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    return submission_df\n",
        "\n",
        "# Warmup function for consistent timing\n",
        "def warmup_model(model, tokenizer, device):\n",
        "    \"\"\"Warm up the model and CUDA kernels for consistent timing\"\"\"\n",
        "    print(\"Warming up model and CUDA kernels...\")\n",
        "    dummy_text = \"summarize: This is a warmup text for optimal performance.\"\n",
        "    dummy_inputs = tokenizer(dummy_text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
        "    dummy_inputs = {k: v.to(device) for k, v in dummy_inputs.items()}\n",
        "\n",
        "    # Run a few warmup iterations\n",
        "    with torch.inference_mode():\n",
        "        for _ in range(3):\n",
        "            _ = model.generate(**dummy_inputs, max_length=32, num_beams=1)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "    print(\"Warmup completed!\")\n",
        "\n",
        "# Execute ultra-fast prediction\n",
        "submission = generate_test_predictions()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dd33a16",
      "metadata": {
        "id": "2dd33a16"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Summarize the training process and results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d0bb5c8",
      "metadata": {
        "id": "0d0bb5c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Summary of Clinical Reasoning Model Training\n",
            "==================================================\n",
            "\n",
            "1. Model Architecture\n",
            "Base model: t5-base\n",
            "Parameters: 222,903,552\n",
            "Source length: 512, Target length: 128\n",
            "Training batch size: 4 with 8x gradient accumulation\n",
            "\n",
            "2. Training Summary\n",
            "Training duration: 11.85 minutes\n",
            "Training examples: 320\n",
            "Validation examples: 80\n",
            "Test examples: 100\n",
            "\n",
            "3. Performance Metrics\n",
            "ROUGE-1: 0.3923\n",
            "ROUGE-2: 0.1893\n",
            "ROUGE-L: 0.3118\n",
            "BLEU: 0.1341\n",
            "\n",
            "6. Key Findings and Observations\n",
            "- The model demonstrates ability to generate clinically relevant assessments\n",
            "- Performance varies with input complexity and medical specificity\n",
            "- Longer responses tend to have better ROUGE scores but may include irrelevant information\n",
            "- Cross-validation confirms model stability across different data subsets\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "7. Recommendations for Further Improvement\n",
            "- Larger dataset with more diverse clinical scenarios would improve generalization\n",
            "- Fine-tuning with domain-specific medical corpora could enhance medical terminology usage\n",
            "- Experiment with other model architectures (FLAN-T5, Llama-2, etc.)\n",
            "- Implement post-processing to improve formatting and clinical terminology\n",
            "- Incorporate structured clinical knowledge for more accurate assessments\n",
            "\n",
            "Training complete! The model is ready for clinical reasoning assessment generation.\n"
          ]
        }
      ],
      "source": [
        "# Conclusion and Final Summary\n",
        "\n",
        "def summarize_results():\n",
        "    \"\"\"Summarize the training process and results\"\"\"\n",
        "    print(\"Final Summary of Clinical Reasoning Model Training\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Model architecture summary\n",
        "    print(\"\\n1. Model Architecture\")\n",
        "    print(f\"Base model: {MODEL_NAME}\")\n",
        "    print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    print(f\"Source length: {MAX_SOURCE_LENGTH}, Target length: {MAX_TARGET_LENGTH}\")\n",
        "    print(f\"Training batch size: {BATCH_SIZE} with {GRADIENT_ACCUMULATION_STEPS}x gradient accumulation\")\n",
        "\n",
        "    # Training summary\n",
        "    print(\"\\n2. Training Summary\")\n",
        "    print(f\"Training duration: {train_duration / 60:.2f} minutes\")\n",
        "    print(f\"Training examples: {len(train_subset)}\")\n",
        "    print(f\"Validation examples: {len(val_subset)}\")\n",
        "    print(f\"Test examples: {len(test_df)}\")\n",
        "\n",
        "    # Performance metrics\n",
        "    print(\"\\n3. Performance Metrics\")\n",
        "    if 'eval_rouge1' in eval_results:\n",
        "        print(f\"ROUGE-1: {eval_results['eval_rouge1']:.4f}\")\n",
        "    if 'eval_rouge2' in eval_results:\n",
        "        print(f\"ROUGE-2: {eval_results['eval_rouge2']:.4f}\")\n",
        "    if 'eval_rougeL' in eval_results:\n",
        "        print(f\"ROUGE-L: {eval_results['eval_rougeL']:.4f}\")\n",
        "    if 'eval_bleu' in eval_results:\n",
        "        print(f\"BLEU: {eval_results['eval_bleu']:.4f}\")\n",
        "\n",
        "    # Cross-validation results if available\n",
        "    if 'cv_results' in globals() and cv_results:\n",
        "        print(\"\\n4. Cross-Validation Results\")\n",
        "        for key, value in cv_results.items():\n",
        "            if key.startswith('eval_'):\n",
        "                metric_name = key[5:]  # Remove 'eval_' prefix\n",
        "                print(f\"{metric_name}: {value:.4f}\")\n",
        "\n",
        "    # Check if ensemble results exist\n",
        "    if 'ensemble_results' in globals() and ensemble_results:\n",
        "        print(\"\\n5. Ensemble Model Performance\")\n",
        "        if 'ensemble_scores' in ensemble_results:\n",
        "            avg_score = sum(ensemble_results['ensemble_scores']) / len(ensemble_results['ensemble_scores'])\n",
        "            print(f\"Ensemble ROUGE-L: {avg_score:.4f}\")\n",
        "\n",
        "    # Key findings\n",
        "    print(\"\\n6. Key Findings and Observations\")\n",
        "    print(\"- The model demonstrates ability to generate clinically relevant assessments\")\n",
        "    print(\"- Performance varies with input complexity and medical specificity\")\n",
        "    print(\"- Longer responses tend to have better ROUGE scores but may include irrelevant information\")\n",
        "    print(\"- Cross-validation confirms model stability across different data subsets\")\n",
        "\n",
        "    # Generate visualization of final results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Plot metrics\n",
        "    metrics = {\n",
        "        'ROUGE-1': eval_results.get('eval_rouge1', 0),\n",
        "        'ROUGE-2': eval_results.get('eval_rouge2', 0),\n",
        "        'ROUGE-L': eval_results.get('eval_rougeL', 0),\n",
        "        'BLEU': eval_results.get('eval_bleu', 0)\n",
        "    }\n",
        "\n",
        "    plt.bar(metrics.keys(), metrics.values())\n",
        "    plt.title('Final Model Performance Metrics')\n",
        "    plt.ylabel('Score')\n",
        "    plt.ylim(0, 1.0)\n",
        "\n",
        "    for i, (key, value) in enumerate(metrics.items()):\n",
        "        plt.text(i, value + 0.02, f'{value:.3f}', ha='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Recommendations\n",
        "    print(\"\\n7. Recommendations for Further Improvement\")\n",
        "    print(\"- Larger dataset with more diverse clinical scenarios would improve generalization\")\n",
        "    print(\"- Fine-tuning with domain-specific medical corpora could enhance medical terminology usage\")\n",
        "    print(\"- Experiment with other model architectures (FLAN-T5, Llama-2, etc.)\")\n",
        "    print(\"- Implement post-processing to improve formatting and clinical terminology\")\n",
        "    print(\"- Incorporate structured clinical knowledge for more accurate assessments\")\n",
        "\n",
        "    return\n",
        "\n",
        "# Generate final summary\n",
        "summarize_results()\n",
        "\n",
        "print(\"\\nTraining complete! The model is ready for clinical reasoning assessment generation.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
